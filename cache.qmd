# Cache

We use cache to avoid having to repeat time-consuming calculations when they will return the same result each time.   To understand how, you should start with an [introduction to cache in RMarkdown](https://bookdown.org/yihui/rmarkdown-cookbook/cache.html) that provides an overview of the rationale for caching objects.  However, be aware that the discussion is anchored in the context of using knitr's built in caching abilities, which we don't recommend.   

To use caching effectively, you need to understand when and how to [invalidate the cache](https://yihui.org/en/2018/06/cache-invalidation/).

## Solutions

### `cache = TRUE` (not recommended)

You can set `cache = TRUE` in any specific code chunk to have knitr cache those calculations for your later reuse.  However, we don't recommend this because it makes the process and instances where the cache is invalidated more opaque.   And more importantly, this caching will not be used for interactive use when you send your code chunks to the console as you work live.  

Nonetheless, this approach is [well documented](https://yihui.org/knitr/demo/cache/) including more advanced topics like [paths](https://bookdown.org/yihui/rmarkdown-cookbook/cache-path.html) and [lazy loading](https://bookdown.org/yihui/rmarkdown-cookbook/cache-lazy.html).

### Explicit `write_rds()` (not recommended

You could instead manually save objects that you want to avoid recalculating.  This is a legitimate method that gives you full and transparent control over caching.  It will also work both interactively in the console and when you knit your document.  However, its got a bit more overhead RE the code.  You need to write code to check if the file exists and load it if it does vs. calculate the object if it doesn't.   This is not too hard but it turns out that a function has already been written to handle this overhead for you.  We describe that next.

### `xfun::cache_rds()` (our preferred method)

We believe that the `xfun::cache_rds()` function provides the sweet spot for the balance of control and transparency vs. code overhead.  It also works for both interactive/console and knit workflows.  

Lets demonstrate its use.   We start by setting up some objects that will be used in later time-consuming calculations.
You need to be careful with these objects.  If they ever change, you will need to explicitly invalidate your cached object and re-calculate it.  More on that below.

```{r}
y <- 2
z <- 3
```

Now lets use `y` and `z` in some time consuming set of calculations

* The first argument parameter in `cache_rds()` is the code to execute the time-consuming calculation.  This code is provided to the function inside of curly brackets, `{}`
* Results from `cache_rds()` are assigned to your object (e.g., `x`) as if they came straight from the coded calculations (e.g., instead of `x <- y + z`, we now have `x <- cache_rds({y + z})`).
* We recommend explicitly setting the values for the `dir` and `file` for the cached object.  This way, you control where it is saved and are assured it will be the same location regardless of whether you run this code chunk in the console or knit it.  Initial testing suggested the filename and location will differ for interactive/console vs. knit workflows if you use defaults.  The `/` at the end of the directory name is required to designate this as a folder.  The filename will have the string assigned to `file` as the prefix but will have an additional hash and a `.rds` appended to it as well.
* We recommend explicitly including `rerun = FALSE` as a third parameter.  This provides you an easy way to invalidate the cached object (and a memory aid to consider invalidation when needed).  To invalidate, just set it to `TRUE` and run the chunk again again if any of your globals (e.g., `y`, `z`) have changed (and then set back to FALSE after!).   Alternatively, you can invalidate the cached object by deleting it from the `cache/` folder.
* `cache_rds()` has one additional parameter worth mentioning, `hash`.   You can pass a list of global objects to hash (e.g. `hash = list(y, z)`) that the function will monitor for change.  If any of these globals are re-calculated, it will invalidate your cached object and re-calculate it.  This can be very useful and we should start using it too.  However, our testing suggests that it *may* invalidate the cache in some instances when it shouldn't.  We havent been able to fully document this issue yet. For now, we recommend using this and paying attention to when the cache is incorrectly invalidated (if ever).  Some testing has also suggested that you may need to use >= R version 4.3 to avoid the problem with cache invalidation (but that isn't definite either :-(    We will update this page as we better understand if this is a persistent issue.  
```{r cache_demo}

x <- xfun::cache_rds({
  Sys.sleep(5) # pretend that computations take a while
  y + z
},
rerun = FALSE,
hash = list(y, z),
dir = "cache/",
file = "cache_demo")
```

Now we can use x without recalculating it each time when executing the previous chunk in either console or when knit. Yay!

```{r}
x
```


You can (and should) read the full documentation on [xfun::cache_rds()](https://bookdown.org/yihui/rmarkdown-cookbook/cache-rds.html#cache-rds) prior to using it in your own code.


### Final notes

We have also learned that caching that involves saving an rds file (all of these methods) may encounter problems if you try to cache a keras model object (e.g., via `mlp()` in tidymodels).   To be clear, there is no problem saving resampling statistics from `fit_resamples()` or `tune_grid()`.  The problem is specific to the actual model object returned from `fit()`.   This [issue](https://www.tidyverse.org/blog/2022/09/bundle-0-1-0/) with keras (and perhaps some other types) models is documented and the `bundles` package is designed to solve it.  If you plan to cache (or even just directly save) these model objects, read these docs carefully.   We will eventually work out a piped solution that works to either manually save or use `cache_rds()` with these objects if needed.  Not a high priority for us right now because we dont use keras much yet in our lab.