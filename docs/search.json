[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data Wrangling in the Tidyverse",
    "section": "Welcome",
    "text": "Welcome\nData Wrangling in the Tidyverse\nSplash page provides an overview of the book\n\nR\nmy_list = list(1, 2, 3)\ntypeof(my_list)"
  },
  {
    "objectID": "file_and_path_management.html#use-of-rstudio-projects",
    "href": "file_and_path_management.html#use-of-rstudio-projects",
    "title": "1  Best Practices for File and Path Management",
    "section": "1.1 Use of RStudio Projects",
    "text": "1.1 Use of RStudio Projects\nThe use of RStudio Projects is critical to good managament of your paths and files. When you work within a project, you will have a working directory set within that project (based on where the project files is saved. This working directory can then be combined with relative paths for reading and writing data and other files. It also means that if you share the folders that contain your project (e.g., scripts, data), the paths will continue to work for that colleague as well, regardless of where they situate the folders on their computer.\nWickham et al., describe the rationale and benefits for using projects. Please read this! They also clearly describe the steps to set up a new project so I won’t repeat them here.\nFor our course, we strongly recommend that you set up a project called “iaml”. Inside that root project folder, you can establish a folder for “homework”, and inside that folder you can have sub-folders for each unit (e.g., “unit_2”, “unit_3”). In addition to the homework folder, you might have folders for exams (e.g., “midterm”) and other material that you save (e.g., “pdfs”)."
  },
  {
    "objectID": "file_and_path_management.html#relative-paths",
    "href": "file_and_path_management.html#relative-paths",
    "title": "1  Best Practices for File and Path Management",
    "section": "1.2 Relative Paths",
    "text": "1.2 Relative Paths\nYou should also get in the habit of setting relative paths (relative to your project root) near the start of your script so that you can call those paths easily throughout. Added bonus, if you move those folders within your project, you just need to change one line of code. For example if your raw data and processed data live in separate folders you might have two paths set:\npath_raw <- \"data/raw\"\npath_processed <- \"data/processed\"\nYou can use these path objects with the here() function, which is described next."
  },
  {
    "objectID": "file_and_path_management.html#use-of-here",
    "href": "file_and_path_management.html#use-of-here",
    "title": "1  Best Practices for File and Path Management",
    "section": "1.3 Use of here()",
    "text": "1.3 Use of here()\nhere() in the package here is a replacement for file.path() that we find to work comparable (and better in a few specific circumstances; e.g., when knitting Rmd files). It is very easy to use if you are using Projects as recommended above. That said, it can also be used with the use of Projects, but we dont review that here because you WILL use projects! To use here(), you should load the here package at the top of your script (i.e., library(here)).\nYou can then use here() as a replacement anywhere you would have used file.path()\nFor example, if you want to load a csv file in the folder that you indicated above by path_raw, you could use this line of code:\nd <- read_csv(here(path_raw, \"raw_data.csv\"))\nalternatively, you could supply the relative path directly (though this is not preferred because it can be cumbersome if you move the folder later)\nd <- read_csv(here(\"data/processed\", \"raw_data.csv\"))"
  },
  {
    "objectID": "file_and_path_management.html#sourcing-from-github",
    "href": "file_and_path_management.html#sourcing-from-github",
    "title": "1  Best Practices for File and Path Management",
    "section": "1.4 Sourcing from Github",
    "text": "1.4 Sourcing from Github\nScripts in public repositories on GithHub can be sourced directly from the remote repository on GitHub using source_url() from the `devtools’ package. To do this, follow these steps:\n\nFind the url to the specific file/script you would like to source. This can be done by simply clinical on the file through GitHub in your browser. For example, the url to fun_modeling.R in my lab_support repo is:\n\nhttps://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R\n\nAdd ?raw=true to the end of that url. For example:\n\nhttps://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\n\nPass this url as a string into devtools::source_url()` in your R script. For example:\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")\nIts that easy. Using this method will allow you to continue to use the most up-to-date version of that script even as the repo owner improves it over time. It also doesnt require you to worry about where a local clone of that repo might live on your computer or the computers of anyone with which you share your code."
  },
  {
    "objectID": "file_and_path_management.html#additional-resources",
    "href": "file_and_path_management.html#additional-resources",
    "title": "1  Best Practices for File and Path Management",
    "section": "1.5 Additional Resources",
    "text": "1.5 Additional Resources\n\nBlog with links on the use of projects and here() package\nGood advice for folder management in projects.\nMore good advice on projects and file management"
  },
  {
    "objectID": "iteration.html",
    "href": "iteration.html",
    "title": "2  Iteration",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\ntidymodels_prefer()"
  },
  {
    "objectID": "reproducible_examples.html#why-create-a-reprex",
    "href": "reproducible_examples.html#why-create-a-reprex",
    "title": "3  How to Create a Reproducible Example (Reprex)",
    "section": "3.1 Why create a reprex?",
    "text": "3.1 Why create a reprex?\nCareful use of a reproducible example (a reprex) is critical for…\n\nDebugging your code\nAsking for help on listservs (e.g., Posit/Rstudio community, StackOverflow, and others)\nPosting bug reports for tidymodels or tidyverse packages on GitHub\n\nA reproducible example should be the simplest snippet of code that can reproduce the error that you are encountering. By creating this simple example, it will focus everyone on the nature of your problem(s), reproduce the error message (or incorrect output), and not distract by including code that is irrelevant to your problem."
  },
  {
    "objectID": "reproducible_examples.html#key-featrures-of-a-good-reprex",
    "href": "reproducible_examples.html#key-featrures-of-a-good-reprex",
    "title": "3  How to Create a Reproducible Example (Reprex)",
    "section": "3.2 Key Featrures of a Good Reprex",
    "text": "3.2 Key Featrures of a Good Reprex\nThere are a few key features of a good reprex.\n\nThe reprex should run completely on its own with just copy/paste. It should only require the snippet of shared code to run without any additional scripts, data, etc. This makes it easy for everyone to copy your code into their IDE and run it themselves. This is critical if you want someone else to use their time to help you solve YOUR problem.\nIt should NOT require anyone to source other scripts If you used sourced functions, copy them into the reprex. Or better yet, don’t require those functions at all unless they are the source of the error.\nIt should (almost) never require the person to load data. You should either use fake data (i.e., simulated data) that you create at the start of the reprex or data that you load from the modeldata package (i.e., include library(modeldata) at the top of the reprex).\nRemove all extra lines of code from the reprex that are not necessary to either set up the data or produce the error. Your goal is to use the minimum lines of code needed to reproduce the error.\nInclude only necessary packages. Unfortunately, package conflicts are common in R. By stripping out unnecessary packages from your reprex, you may find the source of a package conflict."
  },
  {
    "objectID": "reproducible_examples.html#debugging-with-your-reprex",
    "href": "reproducible_examples.html#debugging-with-your-reprex",
    "title": "3  How to Create a Reproducible Example (Reprex)",
    "section": "3.3 Debugging with your Reprex",
    "text": "3.3 Debugging with your Reprex\nYou will find that in the process of simplifying your code, you will find the source of your error yourself in many, many instances. This process of cutting extraneous code and simplifying it is the recommended process for debugging your code systematically.\nOnce you have your simple reprex, you can now experiment with it to find the boundary conditions for the error.\n* Read help on the function that seems to generate the error.\n* Check the name of your variables, the function, and the function parameters.\n* Make sure you are assigning the correct argument to each function parameter. You may want to name the parameters formally rather than relying on the order they are listed in the function).\n* Use R debugging tools like traceback and others.\n* Look carefully at your tibble. Consider the class of all the variables. Are missing values causing an issue?"
  },
  {
    "objectID": "reproducible_examples.html#using-reprex-package-to-post-your-reprex",
    "href": "reproducible_examples.html#using-reprex-package-to-post-your-reprex",
    "title": "3  How to Create a Reproducible Example (Reprex)",
    "section": "3.4 Using reprex Package to Post your Reprex",
    "text": "3.4 Using reprex Package to Post your Reprex\nIf you cannot solve the problem after doing substantial testing and debugging within your reprex on your own, it is time to ask for help. In the course, you will ask for help from us (John and the TAs). In the future, you will ask for help from the R community (see options described above). In either instance, be respectful and only ask after you have tried hard to solve the problem on your own and after you’ve truly hit a wall.\nIf you are asking for help, you will need to describe your problem and post your reprex. Try to describe the nature of the issue clearly (though in some instances all that takes is reporting the error message). It might take more description if the problem is not a formal error in the code but incorrect output. The post the reprex. A provides a package (reprex) to help you create markdown code to post your reprex. Here are the steps to use it.\n\nMake sure you have installed the reprex package (install.packages(\"reprex\"); you do not need to load it, only install it).\nCopy our reprex code to your clipboard.\ntype reprex::reprex(venue = \"slack\") (substitute “gh” for slack if you are posting to github, the rstudio/posit community, or StackOverflow).\nAfter the reprex function runs, it will put markdown code onto your clipboard. You can now go back to your post and paste it in. Its that simple!"
  },
  {
    "objectID": "reproducible_examples.html#using-data-in-your-reprex",
    "href": "reproducible_examples.html#using-data-in-your-reprex",
    "title": "3  How to Create a Reproducible Example (Reprex)",
    "section": "3.5 Using Data in your Reprex",
    "text": "3.5 Using Data in your Reprex\nYou should (almost) never need to provide a separate data file with your reprex (and it just makes things more complicated for those trying to help you if you do).\nInstead, you can often simulate a simple data set. Here is a dataset with variables for different classes including numeric, character and factor.\n\nlibrary(dplyr)\nset.seed(12345) # to make data reproducible if that matters\nd <- tibble (x = rnorm(24), \n             y = rep(c(\"group_1\", \"group_2\"), 12), \n             z = factor(rep(c(\"level_1\", \"level_2\", \"level_3\"), 8))) %>% \n  glimpse()\n\nRows: 24\nColumns: 3\n$ x <dbl> 0.5855288, 0.7094660, -0.1093033, -0.4534972, 0.6058875, -1.8179560,…\n$ y <chr> \"group_1\", \"group_2\", \"group_1\", \"group_2\", \"group_1\", \"group_2\", \"g…\n$ z <fct> level_1, level_2, level_3, level_1, level_2, level_3, level_1, level…\n\nd %>% print()\n\n# A tibble: 24 × 3\n        x y       z      \n    <dbl> <chr>   <fct>  \n 1  0.586 group_1 level_1\n 2  0.709 group_2 level_2\n 3 -0.109 group_1 level_3\n 4 -0.453 group_2 level_1\n 5  0.606 group_1 level_2\n 6 -1.82  group_2 level_3\n 7  0.630 group_1 level_1\n 8 -0.276 group_2 level_2\n 9 -0.284 group_1 level_3\n10 -0.919 group_2 level_1\n# ℹ 14 more rows\n\n\nThis should get you started with some edits to fit your scenario. For other situations, the functions caret::twoClassSim() or caret::SLC14_1() might be good tools to simulate data for you.\nAlternatively, you can include library(modeldata) at the top of your reprex and then have access to all the datasets in that package. You can see a list of the dataset names for the repo on Github.\names might be a good starting point for you for fake data because it has lots of options for variables. I find the variable names a bit annoying (because they are not snake_case) but that shouldn’t be an issue for a simple reprex. You should probably select down to just the variables you need and slice out a random sample of fewer cases for use in your reprex. For example…\n\nlibrary(modeldata)\names_1 <- ames %>% \n  select(Sale_Price, Lot_Config, Lot_Frontage) %>% \n  slice_sample(n = 50) %>% \n  glimpse()\n\nRows: 50\nColumns: 3\n$ Sale_Price   <int> 146000, 755000, 115000, 160000, 133000, 187000, 285000, 1…\n$ Lot_Config   <fct> Inside, Corner, Inside, Corner, Inside, Inside, Inside, C…\n$ Lot_Frontage <dbl> 24, 104, 100, 87, 34, 65, 0, 0, 0, 70, 160, 66, 50, 59, 5…"
  },
  {
    "objectID": "reproducible_examples.html#more-help",
    "href": "reproducible_examples.html#more-help",
    "title": "3  How to Create a Reproducible Example (Reprex)",
    "section": "3.6 More Help",
    "text": "3.6 More Help\nFor more help see:\n\nHow to use a Reprex from tidyverse folks\n[What is a reprex] (https://github.com/tidyverse/reprex#what-is-a-reprex) from tidyverse folks.\nHow to make a great R reproducible example from Stack Overflow *How to create a reprex from Stack Overflow\nA good example bug report from tidymodels on Github: #46"
  },
  {
    "objectID": "parallel_processing.html#tune_grid-in-tidymodels",
    "href": "parallel_processing.html#tune_grid-in-tidymodels",
    "title": "4  Parallel Processing",
    "section": "4.1 tune_grid() in tidymodels",
    "text": "4.1 tune_grid() in tidymodels\nSet up data, resamples, recipe, tuning grid. Will do 3x 10-fold CV to tune an elasticnet glm with a sample size of 1000 and 30 features\n\n# set up data\nn_obs <- 1000\nn_x <- 30\nirr_err <- 5\nd <- MASS::mvrnorm(n = n_obs, mu = rep(0,n_x), Sigma = diag(n_x)) %>% \n    magrittr::set_colnames(str_c(\"x\", 1:n_x)) %>% \n    as_tibble() %>% \n    mutate(error = rnorm(n_obs, 0, irr_err),\n           y = rowSums(across(everything()))) %>% \n    select(-error)\n\n# recipe\nrec <- recipe(y ~ ., data = d)\n\n# 3x 10-fold CV\nset.seed(19690127)\nsplits <- d %>% \n  vfold_cv(v = 10, repeats = 3, strata = \"y\")\n\n# tuning grid\ntune_grid <- expand_grid(penalty = exp(seq(0, 6, length.out = 200)),\n                           mixture = seq(0, 1, length.out = 11))\n\nFirst, let’s benchmark without parallel processing. tune_grid() (and fit_resamples()) default is to allow parallel processing so have to explicitly turn it off using control_grid(). You will NOT do this. It is only to show the benefits of parallel processing.\n\ntic()\nlinear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\") %>% \n  tune_grid(preprocessor = rec, \n            resamples = splits, grid = tune_grid, \n            metrics = metric_set(rmse),\n            control = control_grid(allow_par = FALSE)) # turn off pp\n\n# Tuning results\n# 10-fold cross-validation repeated 3 times using stratification \n# A tibble: 30 × 5\n   splits            id      id2    .metrics             .notes          \n   <list>            <chr>   <chr>  <list>               <list>          \n 1 <split [900/100]> Repeat1 Fold01 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 2 <split [900/100]> Repeat1 Fold02 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 3 <split [900/100]> Repeat1 Fold03 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 4 <split [900/100]> Repeat1 Fold04 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 5 <split [900/100]> Repeat1 Fold05 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 6 <split [900/100]> Repeat1 Fold06 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 7 <split [900/100]> Repeat1 Fold07 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 8 <split [900/100]> Repeat1 Fold08 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 9 <split [900/100]> Repeat1 Fold09 <tibble [2,200 × 6]> <tibble [0 × 3]>\n10 <split [900/100]> Repeat1 Fold10 <tibble [2,200 × 6]> <tibble [0 × 3]>\n# ℹ 20 more rows\n\ntoc()\n\n44.9 sec elapsed\n\n\nNow allow use of parallel processing (the default). No plan is needed here (consistent with findings for foreach()). Yay!\n\ntic()\nlinear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\") %>% \n  tune_grid(preprocessor = rec, \n            resamples = splits, grid = tune_grid, \n            metrics = metric_set(rmse))\n\n# Tuning results\n# 10-fold cross-validation repeated 3 times using stratification \n# A tibble: 30 × 5\n   splits            id      id2    .metrics             .notes          \n   <list>            <chr>   <chr>  <list>               <list>          \n 1 <split [900/100]> Repeat1 Fold01 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 2 <split [900/100]> Repeat1 Fold02 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 3 <split [900/100]> Repeat1 Fold03 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 4 <split [900/100]> Repeat1 Fold04 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 5 <split [900/100]> Repeat1 Fold05 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 6 <split [900/100]> Repeat1 Fold06 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 7 <split [900/100]> Repeat1 Fold07 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 8 <split [900/100]> Repeat1 Fold08 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 9 <split [900/100]> Repeat1 Fold09 <tibble [2,200 × 6]> <tibble [0 × 3]>\n10 <split [900/100]> Repeat1 Fold10 <tibble [2,200 × 6]> <tibble [0 × 3]>\n# ℹ 20 more rows\n\ntoc()\n\n19.31 sec elapsed"
  },
  {
    "objectID": "parallel_processing.html#final-notes",
    "href": "parallel_processing.html#final-notes",
    "title": "4  Parallel Processing",
    "section": "4.2 Final notes",
    "text": "4.2 Final notes\nThe following is often found as an alternative setup for a back-end for parallel processing. It works for future_map() (when conbined with plan) and for foreach() but not in the tidymodels implementations of resampling. Not clear why since those use foreach() but this should not be used if you plan to use tidymodels resampling.\n\nlibrary(doFuture)\nregisterDoFuture()\n\nI tried this with both directly and with various options of plan()\n\n# plan(multisession, workers = parallel::detectCores(logical = FALSE))\n\nand with\n\n# cl <- makeCluster(parallel::detectCores(logical = FALSE))\n# plan(cluster, workers = cl)"
  },
  {
    "objectID": "cache.html#solutions",
    "href": "cache.html#solutions",
    "title": "5  Cache",
    "section": "5.1 Solutions",
    "text": "5.1 Solutions\n\n5.1.1 cache = TRUE (not recommended)\nYou can set cache = TRUE in any specific code chunk to have knitr cache those calculations for your later reuse. However, we don’t recommend this because it makes the process and instances where the cache is invalidated more opaque. And more importantly, this caching will not be used for interactive use when you send your code chunks to the console as you work live.\nNonetheless, this approach is well documented including more advanced topics like paths and lazy loading.\n\n\n5.1.2 Explicit write_rds() (not recommended\nYou could instead manually save objects that you want to avoid recalculating. This is a legitimate method that gives you full and transparent control over caching. It will also work both interactively in the console and when you knit your document. However, its got a bit more overhead RE the code. You need to write code to check if the file exists and load it if it does vs. calculate the object if it doesn’t. This is not too hard but it turns out that a function has already been written to handle this overhead for you. We describe that next.\n\n\n5.1.3 xfun::cache_rds() (our preferred method)\nWe believe that the xfun::cache_rds() function provides the sweet spot for the balance of control and transparency vs. code overhead. It also works for both interactive/console and knit workflows.\nLets demonstrate its use. We start by setting up some objects that will be used in later time-consuming calculations. You need to be careful with these objects. If they ever change, you will need to explicitly invalidate your cached object and re-calculate it. More on that below.\n\ny <- 2\nz <- 3\n\nNow lets use y and z in some time consuming set of calculations\n\nThe first argument parameter in cache_rds() is the code to execute the time-consuming calculation. This code is provided to the function inside of curly brackets, {}\nResults from cache_rds() are assigned to your object (e.g., x) as if they came straight from the coded calculations (e.g., instead of x <- y + z, we now have x <- cache_rds({y + z})).\nWe recommend explicitly setting the values for the dir and file for the cached object. This way, you control where it is saved and are assured it will be the same location regardless of whether you run this code chunk in the console or knit it. Initial testing suggested the filename and location will differ for interactive/console vs. knit workflows if you use defaults. The / at the end of the directory name is required to designate this as a folder. The filename will have the string assigned to file as the prefix but will have an additional hash and a .rds appended to it as well.\nWe recommend explicitly including rerun = FALSE as a third parameter. This provides you an easy way to invalidate the cached object (and a memory aid to consider invalidation when needed). To invalidate, just set it to TRUE and run the chunk again again if any of your globals (e.g., y, z) have changed (and then set back to FALSE after!). Alternatively, you can invalidate the cached object by deleting it from the cache/ folder.\ncache_rds() has one additional parameter worth mentioning, hash. You can pass a list of global objects to hash (e.g. hash = list(y, z)) that the function will monitor for change. If any of these globals are re-calculated, it will invalidate your cached object and re-calculate it. This can be very useful and we should start using it too. However, our testing suggests that it may invalidate the cache in some instances when it shouldn’t. We havent been able to fully document this issue yet. For now, we recommend using this and paying attention to when the cache is incorrectly invalidated (if ever). Some testing has also suggested that you may need to use >= R version 4.3 to avoid the problem with cache invalidation (but that isn’t definite either :-( We will update this page as we better understand if this is a persistent issue.\n\n\nx <- xfun::cache_rds({\n  Sys.sleep(5) # pretend that computations take a while\n  y + z\n},\nrerun = FALSE,\nhash = list(y, z),\ndir = \"cache/\",\nfile = \"cache_demo\")\n\nNow we can use x without recalculating it each time when executing the previous chunk in either console or when knit. Yay!\n\nx\n\n[1] 5\n\n\nYou can (and should) read the full documentation on xfun::cache_rds() prior to using it in your own code.\n\n\n5.1.4 Final notes\n\n5.1.4.1 Cache and Github\nWe obviously don’t want our cached objects getting added to our repos. This could make the repos become too large and/or add sensitive data to them. It is easy to avoid this though. Assuming you are calling the folder that you will save the cached files to cache/ as recommended above, just add the following line to your .gitignore file\n*_cache/\n\n\n5.1.4.2 Saving model objects\nWe have also learned that caching that involves saving an rds file (all of these methods) may encounter problems if you try to cache a keras model object (e.g., via mlp() in tidymodels). To be clear, there is no problem saving resampling statistics from fit_resamples() or tune_grid(). The problem is specific to the actual model object returned from fit(). This issue with keras (and perhaps some other types) models is documented and the bundles package is designed to solve it. If you plan to cache (or even just directly save) these model objects, read these docs carefully. We will eventually work out a piped solution that works to either manually save or use cache_rds() with these objects if needed. Not a high priority for us right now because we dont use keras much yet in our lab."
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "6  Simulations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\ntidymodels_prefer()"
  },
  {
    "objectID": "programming.html#reading-csv-files",
    "href": "programming.html#reading-csv-files",
    "title": "7  Programming Notes",
    "section": "7.1 Reading csv files",
    "text": "7.1 Reading csv files\nWe typically save our data as csv files. There are many benefits to this format (e.g., easy to share, easy to view outside of R) but one downside is that the don’t store information about variable/column class. We need to establish the appropriate class for each column when we read the data.\n\n7.1.1 Using col_types()\nIf possible, it is best to set the class for each column/variable specifically using the col_types() parameter in read_csv() This forces you to specifically examine and consider each column to decide its class (e.g., is a column with numbers best set as numeric or ordered factor) and the levels if its class is nominal. Of course, this is part of cleaning EDA so you should have done this when you first started working with the data.\nRe-classing is typically needed to convert raw character columns to factor (ordered or unordered) and sometimes to convert raw numeric columns to factor (likely ordered, e.g., a likert scale).\nHere is an example using the cars dataset\n\ndf <- read_csv(here(path_data, \"auto_trn.csv\"),\n               col_type = list(mpg = col_factor(levels = c(\"low\", \"high\")),\n                               # here we handle cylinders as an ordered factor\n                               cylinders = col_factor(levels = \n                                                        as.character(c(3,4,5,6,8)), \n                                                      ordered = TRUE),   \n                               displacement = col_double(),\n                               horsepower = col_double(),\n                               weight = col_double(),\n                               acceleration = col_double(),\n                               year = col_double(),\n                               origin = col_factor(levels = \n                                                     c(\"american\", \n                                                       \"japanese\", \n                                                       \"european\")))) %>% \n  glimpse()\n\nRows: 294\nColumns: 8\n$ mpg          <fct> high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    <ord> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement <dbl> 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   <dbl> 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       <dbl> 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration <dbl> 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       <fct> japanese, japanese, european, european, european, europea…\n\n\n\n\n7.1.2 Using a separate mutate()\nIn some instances (e.g., data file with very large number of variables, very consistently organized data character data is well-behaved), you may want to read the data in first and then use mutate() to change classes as needed.\nIn these instances, we prefer to set the col_types() parameter to cols() to prevent the verbose message about column classes.\nHere is an example using the ames dataset with all predictors. To start, we only re-class all character columns to unordered factor and one numeric column to an ordered factor. As we work with the data (during cleaning EDA), we may decide that there are other columns that need to be re-classed. If so, we could add additional lines to the mutuate()\n\npath_unit6 <- \"homework/unit_6\"\ndf <- read_csv(here(path_data, \"ames_full_cln.csv\"),\n               col_types = cols()) %>% \n  # convert all character to unordered factors\n  mutate(across(where(is.character), as_factor),\n         overall_qual = ordered(overall_qual, levels = as.character(1:10))) %>% \n  glimpse()\n\nRows: 1,955\nColumns: 81\n$ pid             <fct> x0526301100, x0526350040, x0526351010, x0527105010, x0…\n$ ms_sub_class    <fct> x020, x020, x020, x060, x120, x120, x120, x060, x060, …\n$ ms_zoning       <fct> rl, rh, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl…\n$ lot_frontage    <dbl> 141, 80, 81, 74, 41, 43, 39, 60, 75, 63, 85, NA, 47, 1…\n$ lot_area        <dbl> 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10…\n$ street          <fct> pave, pave, pave, pave, pave, pave, pave, pave, pave, …\n$ alley           <fct> none, none, none, none, none, none, none, none, none, …\n$ lot_shape       <fct> ir1, reg, ir1, ir1, reg, ir1, ir1, reg, ir1, ir1, reg,…\n$ land_contour    <fct> lvl, lvl, lvl, lvl, lvl, hls, lvl, lvl, lvl, lvl, lvl,…\n$ utilities       <fct> all_pub, all_pub, all_pub, all_pub, all_pub, all_pub, …\n$ lot_config      <fct> corner, inside, corner, inside, inside, inside, inside…\n$ land_slope      <fct> gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl,…\n$ neighborhood    <fct> n_ames, n_ames, n_ames, gilbert, stone_br, stone_br, s…\n$ condition_1     <fct> norm, feedr, norm, norm, norm, norm, norm, norm, norm,…\n$ condition_2     <fct> norm, norm, norm, norm, norm, norm, norm, norm, norm, …\n$ bldg_type       <fct> one_fam, one_fam, one_fam, one_fam, twhs_ext, twhs_ext…\n$ house_style     <fct> x1story, x1story, x1story, x2story, x1story, x1story, …\n$ overall_qual    <ord> 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, …\n$ overall_cond    <dbl> 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 2, 5, 6, 6, …\n$ year_built      <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, …\n$ year_remod_add  <dbl> 1960, 1961, 1958, 1998, 2001, 1992, 1996, 1999, 1994, …\n$ roof_style      <fct> hip, gable, hip, gable, gable, gable, gable, gable, ga…\n$ roof_matl       <fct> comp_shg, comp_shg, comp_shg, comp_shg, comp_shg, comp…\n$ exterior_1st    <fct> brk_face, vinyl_sd, wd_sdng, vinyl_sd, cemnt_bd, hd_bo…\n$ exterior_2nd    <fct> plywood, vinyl_sd, wd_sdng, vinyl_sd, cment_bd, hd_boa…\n$ mas_vnr_type    <fct> stone, none, brk_face, none, none, none, none, none, n…\n$ mas_vnr_area    <dbl> 112, 0, 108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 603, 0, 350, 0…\n$ exter_qual      <fct> ta, ta, ta, ta, gd, gd, gd, ta, ta, ta, ta, gd, ex, gd…\n$ exter_cond      <fct> ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ foundation      <fct> c_block, c_block, c_block, p_conc, p_conc, p_conc, p_c…\n$ bsmt_qual       <fct> ta, ta, ta, gd, gd, gd, gd, ta, gd, gd, gd, gd, gd, gd…\n$ bsmt_cond       <fct> gd, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ bsmt_exposure   <fct> gd, no, no, no, mn, no, no, no, no, no, gd, av, gd, av…\n$ bsmt_fin_type_1 <fct> blq, rec, alq, glq, glq, alq, glq, unf, unf, unf, glq,…\n$ bsmt_fin_sf_1   <dbl> 639, 468, 923, 791, 616, 263, 1180, 0, 0, 0, 637, 368,…\n$ bsmt_fin_type_2 <fct> unf, lw_q, unf, unf, unf, unf, unf, unf, unf, unf, unf…\n$ bsmt_fin_sf_2   <dbl> 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0, 0, 0, 0, 1…\n$ bsmt_unf_sf     <dbl> 441, 270, 406, 137, 722, 1017, 415, 994, 763, 789, 663…\n$ total_bsmt_sf   <dbl> 1080, 882, 1329, 928, 1338, 1280, 1595, 994, 763, 789,…\n$ heating         <fct> gas_a, gas_a, gas_a, gas_a, gas_a, gas_a, gas_a, gas_a…\n$ heating_qc      <fct> fa, ta, ta, gd, ex, ex, ex, gd, gd, gd, gd, ta, ex, gd…\n$ central_air     <fct> y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, …\n$ electrical      <fct> s_brkr, s_brkr, s_brkr, s_brkr, s_brkr, s_brkr, s_brkr…\n$ x1st_flr_sf     <dbl> 1656, 896, 1329, 928, 1338, 1280, 1616, 1028, 763, 789…\n$ x2nd_flr_sf     <dbl> 0, 0, 0, 701, 0, 0, 0, 776, 892, 676, 0, 0, 1589, 672,…\n$ low_qual_fin_sf <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gr_liv_area     <dbl> 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1…\n$ bsmt_full_bath  <dbl> 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, …\n$ bsmt_half_bath  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ full_bath       <dbl> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2, …\n$ half_bath       <dbl> 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, …\n$ bedroom_abv_gr  <dbl> 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 4, 4, 1, 2, 3, 3, …\n$ kitchen_abv_gr  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ kitchen_qual    <fct> ta, ta, gd, ta, gd, gd, gd, gd, ta, ta, gd, gd, ex, ta…\n$ tot_rms_abv_grd <dbl> 7, 5, 6, 6, 6, 5, 5, 7, 7, 7, 5, 4, 12, 8, 8, 4, 7, 7,…\n$ functional      <fct> typ, typ, typ, typ, typ, typ, typ, typ, typ, typ, typ,…\n$ fireplaces      <dbl> 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, …\n$ fireplace_qu    <fct> gd, none, none, ta, none, none, ta, ta, ta, gd, po, no…\n$ garage_type     <fct> attchd, attchd, attchd, attchd, attchd, attchd, attchd…\n$ garage_yr_blt   <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, …\n$ garage_finish   <fct> fin, unf, unf, fin, fin, r_fn, r_fn, fin, fin, fin, un…\n$ garage_cars     <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, …\n$ garage_area     <dbl> 528, 730, 312, 482, 582, 506, 608, 442, 440, 393, 506,…\n$ garage_qual     <fct> ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ garage_cond     <fct> ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ paved_drive     <fct> p, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, …\n$ wood_deck_sf    <dbl> 210, 140, 393, 212, 0, 0, 237, 140, 157, 0, 192, 0, 50…\n$ open_porch_sf   <dbl> 62, 0, 36, 34, 0, 82, 152, 60, 84, 75, 0, 54, 36, 12, …\n$ enclosed_porch  <dbl> 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ x3ssn_porch     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ screen_porch    <dbl> 0, 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 140, 210, 0, 0, 0…\n$ pool_area       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ pool_qc         <fct> none, none, none, none, none, none, none, none, none, …\n$ fence           <fct> none, mn_prv, none, mn_prv, none, none, none, none, no…\n$ misc_feature    <fct> none, none, gar2, none, none, none, none, none, none, …\n$ misc_val        <dbl> 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mo_sold         <dbl> 5, 6, 6, 3, 4, 1, 3, 6, 4, 5, 2, 6, 6, 6, 6, 6, 2, 1, …\n$ yr_sold         <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, …\n$ sale_type       <fct> wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd…\n$ sale_condition  <fct> normal, normal, normal, normal, normal, normal, normal…\n$ sale_price      <dbl> 215000, 105000, 172000, 189900, 213500, 191500, 236500…"
  },
  {
    "objectID": "programming.html#iteration",
    "href": "programming.html#iteration",
    "title": "7  Programming Notes",
    "section": "7.2 Iteration",
    "text": "7.2 Iteration\n\n7.2.1 across()\n\n\n7.2.2 map() and future_map()\n\n\n7.2.3 for loops\n\n\n7.2.4 foreach loops\n\n\n7.2.5 Nesting\nSome useful tutorials\n\nhttps://r4ds.had.co.nz/many-models.html\nhttps://bookdown.org/Maxine/r4ds/nesting.html\nhttps://tidyr.tidyverse.org/reference/nest.html"
  },
  {
    "objectID": "programming.html#advanced-data-wrangling",
    "href": "programming.html#advanced-data-wrangling",
    "title": "7  Programming Notes",
    "section": "7.3 Advanced data wrangling",
    "text": "7.3 Advanced data wrangling\nSee this article for details on the use of dplry for row-wise operations\nSee this article for details on the use of dplry for column-wise operations\nThe tidy folks have also written many more articles on dplyr that are worth a look"
  },
  {
    "objectID": "programming.html#tidy-programming",
    "href": "programming.html#tidy-programming",
    "title": "7  Programming Notes",
    "section": "7.4 Tidy programming",
    "text": "7.4 Tidy programming\nProgramming with dplyr can be complicated because of tidy evaluation. This programming vignette provides useful documentation on data masking and tidy selection\nIf you plan to do a lot of R programming, I highly recommend reading chapters 17-21 on meta-programming in R"
  },
  {
    "objectID": "quarto.html#general",
    "href": "quarto.html#general",
    "title": "8  Quarto",
    "section": "8.1 General",
    "text": "8.1 General\nComprehensive guide to Quarto\n\n8.1.1 Chunk options\nChunk options are now specified inside the r backtics, with the following syntax:\n#| echo: false\n#| warning: false\nTo specify these options globally, they need to be added to the _quarto.yml file, with these lines:\nexecute:\n   echo: false\n   warning: false\n\n\n8.1.2 Tables\n\n\n\n\n\nMake\nModel\nMiles Per Gallon\nCylinders\nGears\n\n\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nDatsun\n710\n22.8\n4\n4\n\n\nHornet\n4\n21.4\n6\n3\n\n\nHornet\nSportabout\n18.7\n8\n3\n\n\n\n\n\nPlain kable() renders a nicely striped in HTML. It does not allow grouping in the way kable_extra does. However, when you call kableExtra, it does away with the nice striping and spacing and you then have to define that explicitly!\n\n\n\n\n \n  \n    Make \n    Model \n    Miles Per Gallon \n    Cylinders \n    Gears \n  \n \n\n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Datsun \n    710 \n    22.8 \n    4 \n    4 \n  \n  \n    Hornet \n    4 \n    21.4 \n    6 \n    3 \n  \n  \n    Hornet \n    Sportabout \n    18.7 \n    8 \n    3 \n  \n\n\n\n\n\nNote the table above, although the code is identical to the first table, has lost all formatting thanks to kableExtra. The table below re-produces that via kable_styling (with additional features such as floating around text, and footnotes; although it seems the float doesn’t actually work!).\nFor more advanced tables & figures see Appendix A which reproduces the tables & figures from the 2023 EMA paper.\nTo be sure tables display with NA cells as blanks (instead of “NA”) include this before your first table:\noptions(knitr.kable.NA = \"\")\nSide note, a useful kableExtra function is collapse_rows, yet that seems to be persistently broken\nrow_spec() has a parameter, hline_after, which, ostensibly, should create a horizontal line under that row, right? Apparently it only works on latex although this is undocumented. Apparently the solution is to add row_spec(X, extra_css = \"border-bottom: 1px solid\")) (where x is the rownum; 0 for the header, nrow(df) for the last row)\nSimilarly, to supress hlines on headers created by pack_rows(), you need to add extra_css = \"border-bottom: none\" into the pack_rows() command\n\n\n\n\n \n  \n    Make \n    Model \n    Miles Per Gallon \n    Cylinders \n    Gears \n  \n \n\n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Datsun \n    710 \n    22.8 \n    4 \n    4 \n  \n  \n    Hornet \n    4 \n    21.4 \n    6 \n    3 \n  \n  \n    Hornet \n    Sportabout \n    18.7 \n    8 \n    3 \n  \n\n\nNote: \n\n Here is a general comments of the table. \n\n1 Footnote 1; \n\n2 Footnote 2; \n\na Footnote A; \n\nb Footnote B; \n\n* Footnote Symbol 1; \n\n† Footnote Symbol 2\n\n\n\n\n\n\n\n8.1.3 Rendering\nHitting the Render button renders the single active document.\nCall quarto::quarto_render() to render all documents in the active project directory. NB, you must re-render the entire directory if anything changes in the .yml file (such as addition or deletion of a chapter)\nYou can specify render targets and ordering more specifically in project metadata, see https://quarto.org/docs/projects/quarto-projects.html#render-targets\n\n8.1.3.1 PDF\nYou can specify rendering to PDF format by adding the following to your .yml file:\nformat: pdf: documentclass: book\nYou can then explictly render just the PDF format by specifying both the output format and the filename in the call to quarto_render:\nquarto::quarto_render(output_format = “pdf”,output_file = “dwt.pdf”)\nHOWEVER, as of 2023_0424 rendering to PDF in the stable release fails; I’ll attempt to install the dev version and test next"
  },
  {
    "objectID": "quarto.html#documents",
    "href": "quarto.html#documents",
    "title": "8  Quarto",
    "section": "8.2 Documents",
    "text": "8.2 Documents\nWe use Quarto documents for two purposes - reproducible analyses and submitted papers. We will generally render analysis doc to html and papers to pdf. The Quarto Guide provides more detail on on creating pdf and html\nThese documents are styled primarily using Markdown. The Quarto Guide provides more detail on markdown basics"
  },
  {
    "objectID": "quarto.html#papers",
    "href": "quarto.html#papers",
    "title": "8  Quarto",
    "section": "8.3 Papers",
    "text": "8.3 Papers\nTo add a bibliography with a citation style, add the following lines to the _quarto.yml file:\nbibliography: references.bib\ncsl: name_of_csl_file.csl\nNote, we will typically call our csl files from our central lab_support repo; which simply requires the URL of the raw file from github:\ncsl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/national-library-of-medicine-grant-proposals.csl\nThe Quarto Guide provides more detail about how to work with citations and footnotes. As with markdown, the format is [@citekey1; @citekey2] — citations go inside square brackets and are separated by semicolons"
  },
  {
    "objectID": "quarto.html#reproducible-analyses",
    "href": "quarto.html#reproducible-analyses",
    "title": "8  Quarto",
    "section": "8.4 Reproducible Analyses",
    "text": "8.4 Reproducible Analyses"
  },
  {
    "objectID": "quarto.html#presentations",
    "href": "quarto.html#presentations",
    "title": "8  Quarto",
    "section": "8.5 Presentations",
    "text": "8.5 Presentations\nWe use Quarto to make revealjs slide decks for presentations. The Quarto Guide provides extensive documentation and sample slides. You can begin with the overview of presentations across formats (Quarto can also render powerpoint and other formats). Follow this with a revealjs overview and then the revealjs reference chapter. The Quarto Guide also provides a chapter on presenter tools.\nDivs (:::) and spans([]) are used extensively in presentations. An introduction to their use is provided in the markdown basics chapter. The Pandoc manual provides more detail."
  },
  {
    "objectID": "quarto.html#books",
    "href": "quarto.html#books",
    "title": "8  Quarto",
    "section": "8.6 Books",
    "text": "8.6 Books\nInformation on setting up a book: https://quarto.org/docs/books/\nInformation on setting up Github Pages to publish a book on commit: https://quarto.org/docs/publishing/github-pages.html#render-to-docs"
  },
  {
    "objectID": "quarto.html#publishing",
    "href": "quarto.html#publishing",
    "title": "8  Quarto",
    "section": "8.7 Publishing",
    "text": "8.7 Publishing\nYou can publish documents, books, and presentations to a variety of places\n\n8.7.1 Presentations and Documents\nOur preferred location for presentations (and Quarto Docs) is Quarto Pub. This site is public and free. To use it, you need to set up an account first.\nTo publish a presentation (or other Quarto doc) to Quarto Pub, you should first log in on your default browsers. You should next go to the Terminal tab in the RStudio IDE. Navigate to the folder that contains your presentation. Then type quarto publish quarto-pub. If this is the first time you are publishing at Quarto Pub on that computer, you will need to authorize it. Follow the prompts in the web browser and then in the terminal to complete the publication process. This authorization is saved in a file called _publish.yml, which will be accessed for future updates to the presentation.\nSee additional instructions in the Quarto guide if necessary.\n\n\n8.7.2 Books"
  },
  {
    "objectID": "quarto.html#terminal-commands",
    "href": "quarto.html#terminal-commands",
    "title": "8  Quarto",
    "section": "8.8 Terminal Commands",
    "text": "8.8 Terminal Commands\n\nquarto publish quarto-pub to publish a presentation or document to Quarto Pub"
  },
  {
    "objectID": "quarto_figures_tables.html",
    "href": "quarto_figures_tables.html",
    "title": "Appendix A — Tables & Figures in Quarto",
    "section": "",
    "text": "Appendix for demonstrating complex tables & figures\n\nswitch (Sys.info()[['sysname']],\n        # PC paths\n        Windows = {\n          path_models <- \"P:/studydata/risk/models/ema\"\n          path_data_shared <- \"P:/studydata/risk/data_processed/shared\"\n          path_data_ema <- \"P:/studydata/risk/data_processed/ema\"},\n        # IOS paths\n        Darwin = {\n          path_models <- \"/Volumes/private/studydata/risk/models/ema\"\n          path_data_shared <- \"/Volumes/private/studydata/risk/data_processed/shared\"\n          path_data_ema <- \"/Volumes/private/studydata/risk/data_processed/ema\"}\n       )\n\n\n# Table data\ndisposition <- read_csv(file.path(path_data_ema, \"disposition.csv\"), col_types = \"ccDDcccccccccc\")\nscreen <- read_csv(file.path(path_data_shared, \"screen.csv\"), col_types = vroom::cols()) %>% \n  filter(subid %in% subset(disposition, analysis == \"yes\")$subid)\n\n# Predictions data\npreds_week<- readRDS(file.path(path_models, \"resample_preds_best_all_1week_0_v4_kfold.rds\"))\npreds_day<- readRDS(file.path(path_models, \"resample_preds_best_all_1day_0_v4_kfold.rds\"))\npreds_hour<- readRDS(file.path(path_models, \"resample_preds_best_all_1hour_0_v4_kfold.rds\")) \n\n# posterior probabilites\npp <- readRDS(file.path(path_models, \"posteriors_all_allwindows_0_v4_kfold.rds\"))\n\n# ROC curves\nroc_week <- preds_week %>% \n  roc_curve(prob, truth = truth) %>% \n  mutate(model = \"1week\")\n\nroc_day <- preds_day %>% \n  roc_curve(prob, truth = truth) %>% \n  mutate(model = \"1day\")\n\nroc_hour <- preds_hour%>% \n  roc_curve(prob, truth = truth) %>% \n  mutate(model = \"1hour\")\n\nroc_all <- roc_week %>% \n  bind_rows(roc_day) %>% \n  bind_rows(roc_hour)\n\n# PR curves\npr_week <- preds_week %>% \n  pr_curve(prob, truth = truth) %>% \n  mutate(model = \"1week\")\n\npr_day <- preds_day %>% \n  pr_curve(prob, truth = truth) %>% \n  mutate(model = \"1day\")\n\npr_hour <- preds_hour%>% \n  pr_curve(prob, truth = truth) %>% \n  mutate(model = \"1hour\")\n\npr_all <- pr_week %>% \n  bind_rows(pr_day) %>% \n  bind_rows(pr_hour)\n\n# Grouped SHAPS\nshap_grouped_week <- readRDS(file.path(path_models, \"imp_shap_grouped_all_1week_0_v4.rds\")) %>% \n  group_by(group) %>% \n  summarize(mean_value = mean(abs(shap)), .groups = \"drop\") %>% \n  arrange(mean_value)\nshap_grouped_day <- readRDS(file.path(path_models, \"imp_shap_grouped_all_1day_0_v4.rds\")) %>% \n  group_by(group) %>% \n  summarize(mean_value = mean(abs(shap)), .groups = \"drop\") %>% \n  arrange(mean_value)\nshap_grouped_hour <- readRDS(file.path(path_models, \"imp_shap_grouped_all_1hour_0_v4.rds\")) %>% \n  group_by(group) %>% \n  summarize(mean_value = mean(abs(shap)), .groups = \"drop\") %>% \n  arrange(mean_value)\n\n\n\n\nfootnote_table_dem <- \"N = 151\"\n\n\n\n\n\nDemographics\n \n  \n     \n    N \n    % \n    M \n    SD \n    Range \n  \n \n\n  \n    Age \n     \n     \n    41 \n    11.9 \n    21-72 \n  \n  Sex\n\n    Female \n    74 \n    49.0 \n     \n     \n     \n  \n  \n    Male \n    77 \n    51.0 \n     \n     \n     \n  \n  Race\n\n    American Indian/Alaska Native \n    3 \n    2.0 \n     \n     \n     \n  \n  \n    Asian \n    2 \n    1.3 \n     \n     \n     \n  \n  \n    Black/African American \n    8 \n    5.3 \n     \n     \n     \n  \n  \n    White/Caucasian \n    131 \n    86.8 \n     \n     \n     \n  \n  \n    Other/Multiracial \n    7 \n    4.6 \n     \n     \n     \n  \n  Hispanic, Latino, or Spanish Origin\n\n    Yes \n    4 \n    2.6 \n     \n     \n     \n  \n  \n    No \n    147 \n    97.4 \n     \n     \n     \n  \n  Education\n\n    Less than high school or GED degree \n    1 \n    0.7 \n     \n     \n     \n  \n  \n    High school or GED \n    14 \n    9.3 \n     \n     \n     \n  \n  \n    Some college \n    41 \n    27.2 \n     \n     \n     \n  \n  \n    2-Year degree \n    14 \n    9.3 \n     \n     \n     \n  \n  \n    College degree \n    58 \n    38.4 \n     \n     \n     \n  \n  \n    Advanced degree \n    23 \n    15.2 \n     \n     \n     \n  \n  Employment\n\n    Employed full-time \n    72 \n    47.7 \n     \n     \n     \n  \n  \n    Employed part-time \n    26 \n    17.2 \n     \n     \n     \n  \n  \n    Full-time student \n    7 \n    4.6 \n     \n     \n     \n  \n  \n    Homemaker \n    1 \n    0.7 \n     \n     \n     \n  \n  \n    Disabled \n    7 \n    4.6 \n     \n     \n     \n  \n  \n    Retired \n    8 \n    5.3 \n     \n     \n     \n  \n  \n    Unemployed \n    18 \n    11.9 \n     \n     \n     \n  \n  \n    Temporarily laid off, sick leave, or maternity leave \n    3 \n    2.0 \n     \n     \n     \n  \n  \n    Other, not otherwise specified \n    9 \n    6.0 \n     \n     \n     \n  \n  \n    Personal Income \n     \n     \n    $34,298 \n    $31,807 \n    $0-200,000 \n  \n  Marital Status\n\n    Never married \n    67 \n    44.4 \n     \n     \n     \n  \n  \n    Married \n    32 \n    21.2 \n     \n     \n     \n  \n  \n    Divorced \n    45 \n    29.8 \n     \n     \n     \n  \n  \n    Separated \n    5 \n    3.3 \n     \n     \n     \n  \n  \n    Widowed \n    2 \n    1.3 \n     \n     \n     \n  \n\n\nNote: \n\n N = 151\n\n\n\n\n\n\n\n\nfootnote_table_auh_a <- \"N = 151\"\nfootnote_table_auh_b <- \"Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range.\"\n\n\nauh <- screen %>% \n  summarise(mean = mean(auh_1, na.rm = TRUE),\n            SD = sd(auh_1, na.rm = TRUE),\n            min = min(auh_1, na.rm = TRUE),\n            max = max(auh_1, na.rm = TRUE)) %>% \n  mutate(var = \"Age of first drink\",\n        n = as.numeric(\"\"),\n        perc = as.numeric(\"\")) %>% \n  select(var, n, perc, everything()) %>% \n  full_join(screen %>% \n  summarise(mean = mean(auh_2, na.rm = TRUE),\n            SD = sd(auh_2, na.rm = TRUE),\n            min = min(auh_2, na.rm = TRUE),\n            max = max(auh_2, na.rm = TRUE)) %>% \n  mutate(var = \"Age of regular drinking\",\n        n = as.numeric(\"\"),\n        perc = as.numeric(\"\")) %>% \n  select(var, n, perc, everything()), by = c(\"var\", \"n\", \"perc\", \"mean\", \"SD\", \n                                             \"min\", \"max\")) %>% \n  full_join(screen %>% \n  summarise(mean = mean(auh_3, na.rm = TRUE),\n            SD = sd(auh_3, na.rm = TRUE),\n            min = min(auh_3, na.rm = TRUE),\n            max = max(auh_3, na.rm = TRUE)) %>% \n  mutate(var = \"Age at which drinking became problematic\",\n        n = as.numeric(\"\"),\n        perc = as.numeric(\"\")) %>% \n  select(var, n, perc, everything()), by = c(\"var\", \"n\", \"perc\", \"mean\", \"SD\",\n                                             \"min\", \"max\")) %>% \n  full_join(screen %>% \n  summarise(mean = mean(auh_4, na.rm = TRUE),\n            SD = sd(auh_4, na.rm = TRUE),\n            min = min(auh_4, na.rm = TRUE),\n            max = max(auh_4, na.rm = TRUE)) %>% \n  mutate(var = \"Age of first quit attempt\",\n        n = as.numeric(\"\"),\n        perc = as.numeric(\"\")) %>% \n  select(var, n, perc, everything()), by = c(\"var\", \"n\", \"perc\", \"mean\", \"SD\",\n                                             \"min\", \"max\")) %>% \n  full_join(screen %>% \n  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table\n  filter(auh_5 < 100) %>% \n  summarise(mean = mean(auh_5, na.rm = TRUE),\n            SD = sd(auh_5, na.rm = TRUE),\n            min = min(auh_5, na.rm = TRUE),\n            max = max(auh_5, na.rm = TRUE)) %>% \n  mutate(var = \"Number of Quit Attempts*\",\n        n = as.numeric(\"\"),\n        perc = as.numeric(\"\")) %>% \n  select(var, n, perc, everything()), by = c(\"var\", \"n\", \"perc\", \"mean\", \"SD\",\n                                             \"min\", \"max\")) %>% \n  full_join(screen %>% \n  select(var = auh_6_1) %>%\n  mutate(var = case_when(var == \"Long-Term Residential Treatment (more than 6 months)\" ~ \"Long-term residential (6+ months)\",\n                         TRUE ~ var)) %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = auh_6_2) %>%\n  mutate(var = case_when(var == \"Short-Term Residential Treatment (less than 6 months)\" ~ \"Short-term residential (< 6 months)\",\n                         TRUE ~ var)) %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = auh_6_3) %>%\n  mutate(var = case_when(var == \"Outpatient Treatment\" ~ \"Outpatient\",\n                         TRUE ~ var)) %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = auh_6_4) %>%\n  mutate(var = case_when(var == \"Individual Counseling\" ~ \"Individual counseling\",\n                         TRUE ~ var)) %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = auh_6_5) %>%\n  mutate(var = case_when(var == \"Group Counseling\" ~ \"Group counseling\",\n                         TRUE ~ var)) %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = auh_6_6) %>%\n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = auh_6_7) %>%\n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = auh_7) %>% \n  mutate(var = fct_relevel(factor(var, c(\"Yes\", \"No\")))) %>%\n  group_by(var) %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / sum(n)) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  mutate(across(dsm5_1:dsm5_11, ~ recode(., \"No\" = 0, \"Yes\" = 1))) %>% \n  rowwise() %>% \n  # calculate dsm5 score by adding up dsm5_1 through dsm5_11\n  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, \n                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>% \n  ungroup() %>% \n  summarise(mean = mean(dsm5_total),\n            SD = sd(dsm5_total),\n            min = min(dsm5_total, na.rm = TRUE),\n            max = max(dsm5_total, na.rm = TRUE)) %>% \n  mutate(var = \"Alcohol Use Disorder DSM-5 Symptom Count\",\n        n = as.numeric(\"\"),\n        perc = as.numeric(\"\")) %>% \n  select(var, n, perc, everything()), by = c(\"var\", \"n\", \"perc\", \"mean\", \"SD\",\n                                             \"min\", \"max\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_1) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Tobacco products (cigarettes, chewing tobacco, cigars, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_2) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Cannabis (marijuana, pot, grass, hash, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_3) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Cocaine (coke, crack, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_4) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_5) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Inhalants (nitrous, glue, petrol, paint thinner, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_6) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_7) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) %>% \n  full_join(screen %>% \n  select(var = assist_2_8) %>%\n  filter(var != \"Never\" & !is.na(var)) %>% \n  mutate(var = \"Opioids (heroin, morphine, methadone, codeine, etc.)\") %>% \n  group_by(var) %>% \n  drop_na() %>% \n  summarise(n = n()) %>% \n  mutate(perc = (n / 154) * 100), by = c(\"var\", \"n\", \"perc\")) \n\n# display and format table\nauh %>%\n  mutate(range = str_c(min, \"-\", max)) %>%\n  select(-c(min, max)) %>% \n  kbl(longtable = TRUE,\n      booktabs = TRUE,\n      col.names = c(\"\", \"N\", \"%\", \"M\", \"SD\", \"Range\"),\n      align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n      digits = 1,\n      caption = \"Alcohol Related Information\") %>% \n  kable_styling(bootstrap_options = \"none\",\n                font_size = 12,\n                full_width = TRUE) %>% \n  row_spec(row = 0, align = \"c\", italic = TRUE) %>% \n  pack_rows(\"Alcohol Use Disorder Milestones\", 1, 4, bold = FALSE, label_row_css = \"border-bottom: none;\") %>% \n  pack_rows(\"Lifetime History of Treatment (Can choose more than 1)\", 6, 12, bold = FALSE, label_row_css = \"border-bottom: none;\") %>% \n  pack_rows(\"Received Medication for Alcohol Use Disorder\", 13, 14, bold = FALSE, label_row_css = \"border-bottom: none;\") %>% \n  pack_rows(\"Current (Past 3 Month) Drug Use\", 16, 23, bold = FALSE, label_row_css = \"border-bottom: none;\") %>% \n  footnote(general=footnote_table_auh_a, symbol = c(footnote_table_auh_b), \n           threeparttable = TRUE) %>%\n  row_spec(0, extra_css = \"border-bottom: 1px solid; border-top: 1px solid\") %>% #add borders on the header row\n  row_spec(nrow(auh), extra_css = \"border-bottom: 1px solid\") #add a border after the last row, before the footnote\n\n\n\nAlcohol Related Information\n \n  \n     \n    N \n    % \n    M \n    SD \n    Range \n  \n \n\n  Alcohol Use Disorder Milestones\n\n    Age of first drink \n     \n     \n    14.6 \n    2.9 \n    6-24 \n  \n  \n    Age of regular drinking \n     \n     \n    19.5 \n    6.6 \n    11-56 \n  \n  \n    Age at which drinking became problematic \n     \n     \n    27.8 \n    9.6 \n    15-60 \n  \n  \n    Age of first quit attempt \n     \n     \n    31.5 \n    10.4 \n    15-65 \n  \n  \n    Number of Quit Attempts* \n     \n     \n    5.5 \n    5.8 \n    0-30 \n  \n  Lifetime History of Treatment (Can choose more than 1)\n\n    Long-term residential (6+ months) \n    8 \n    5.2 \n     \n     \n     \n  \n  \n    Short-term residential (< 6 months) \n    49 \n    31.8 \n     \n     \n     \n  \n  \n    Outpatient \n    74 \n    48.1 \n     \n     \n     \n  \n  \n    Individual counseling \n    97 \n    63.0 \n     \n     \n     \n  \n  \n    Group counseling \n    62 \n    40.3 \n     \n     \n     \n  \n  \n    Alcoholics Anonymous/Narcotics Anonymous \n    93 \n    60.4 \n     \n     \n     \n  \n  \n    Other \n    40 \n    26.0 \n     \n     \n     \n  \n  Received Medication for Alcohol Use Disorder\n\n    Yes \n    59 \n    39.1 \n     \n     \n     \n  \n  \n    No \n    92 \n    60.9 \n     \n     \n     \n  \n  \n    Alcohol Use Disorder DSM-5 Symptom Count \n     \n     \n    8.9 \n    1.9 \n    4-11 \n  \n  Current (Past 3 Month) Drug Use\n\n    Tobacco products (cigarettes, chewing tobacco, cigars, etc.) \n    84 \n    54.5 \n     \n     \n     \n  \n  \n    Cannabis (marijuana, pot, grass, hash, etc.) \n    66 \n    42.9 \n     \n     \n     \n  \n  \n    Cocaine (coke, crack, etc.) \n    18 \n    11.7 \n     \n     \n     \n  \n  \n    Amphetamine type stimulants (speed, diet pills, ecstasy, etc.) \n    15 \n    9.7 \n     \n     \n     \n  \n  \n    Inhalants (nitrous, glue, petrol, paint thinner, etc.) \n    3 \n    1.9 \n     \n     \n     \n  \n  \n    Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.) \n    22 \n    14.3 \n     \n     \n     \n  \n  \n    Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.) \n    14 \n    9.1 \n     \n     \n     \n  \n  \n    Opioids (heroin, morphine, methadone, codeine, etc.) \n    16 \n    10.4 \n     \n     \n     \n  \n\n\nNote: \n\n N = 151\n\n* Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range.\n\n\n\n\n\n\n\nfootnote_table_perf_metrics <- \"Insert footnote\"\n\n\nmetrics_week <- preds_week %>%   \n  conf_mat(truth, estimate) %>% \n  summary() %>% \n  mutate(.estimate = round(.estimate, 3)) %>% \n  rename(week = .estimate,\n         metric = .metric) %>% \n  select(-.estimator)\n\nmetrics_day <- preds_day %>%   \n  conf_mat(truth, estimate) %>% \n  summary() %>% \n  mutate(.estimate = round(.estimate, 3)) %>% \n  rename(day = .estimate,\n         metric = .metric) %>% \n  select(-.estimator)\n\nmetrics_hour <- preds_hour %>%   \n  conf_mat(truth, estimate) %>% \n  summary() %>% \n  mutate(.estimate = round(.estimate, 3)) %>% \n  rename(hour = .estimate,\n         metric = .metric) %>% \n  select(-.estimator)\n\nmetrics <- metrics_week %>% \n  full_join(metrics_day, by = \"metric\") %>% \n  full_join(metrics_hour, by = \"metric\") %>% \n  filter(metric %in% c(\"accuracy\", \"sens\", \"spec\", \"ppv\", \"npv\"))\n\nauc <- tibble(metric = \"auc\", \n              week = preds_week %>% roc_auc(prob, truth = truth) %>%  \n                pull(.estimate) %>% round(3), \n              day = preds_day %>% roc_auc(prob, truth = truth) %>%  \n                pull(.estimate) %>% round(3),\n              hour = preds_hour %>% roc_auc(prob, truth = truth) %>%  \n                pull(.estimate) %>% round(3))\n\nmetrics <- metrics %>% \n  bind_rows(auc)\n\nmetrics <- metrics[c(6,1,2,3,4,5),]\n\nmetrics %>%\n kbl(col.names = c(\"Metric\", \"Week\", \"Day\", \"Hour\"),\n      booktabs = TRUE,\n      digits = 3,\n      align = c(\"l\", \"c\", \"c\", \"c\"),\n      caption = \"Performance Metrics by Model\") %>% \n  row_spec(row = 0, align = \"c\") %>% \n  kable_styling(position = \"left\",\n                bootstrap_options = \"none\") %>% \n  footnote(general=footnote_table_perf_metrics) %>%\n  column_spec(1:4, width = \"1in\") %>% #specify column width\n  row_spec(0, extra_css = \"border-bottom: 1px solid; border-top: 1px solid\") %>% #add borders on the header row\n  row_spec(nrow(metrics), extra_css = \"border-bottom: 1px solid\") #add a border after the last row, before the footnote\n\n\n\nPerformance Metrics by Model\n \n  \n    Metric \n    Week \n    Day \n    Hour \n  \n \n\n  \n    auc \n    0.900 \n    0.914 \n    0.933 \n  \n  \n    accuracy \n    0.838 \n    0.849 \n    0.862 \n  \n  \n    sens \n    0.792 \n    0.817 \n    0.841 \n  \n  \n    spec \n    0.853 \n    0.852 \n    0.862 \n  \n  \n    ppv \n    0.646 \n    0.315 \n    0.023 \n  \n  \n    npv \n    0.924 \n    0.982 \n    0.999 \n  \n\n\nNote: \n\n Insert footnote\n\n\n\n\n\n\n\nfig_caption_roc_pp <- \"Insert note here\"\n\n\nroc_plot <- roc_all %>% \n  mutate(model = factor(model, levels = c(\"1week\", \"1day\", \"1hour\"), \n                        labels = c(\"week\", \"day\", \"hour\"))) %>% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  labs(x = \"Specificity\",\n       y = \"Sensitivity\") +\n  scale_x_continuous(breaks = seq(0,1,.25),\n    labels = sprintf(\"%.2f\", seq(1,0,-.25))) +\n  theme(legend.position = \"none\")\n\npp_tidy <- pp %>% \n  tidy(seed = 123)\n\nci <- pp_tidy %>% \n  summary() %>% \n  mutate(model = factor(model, levels = c(\"week\", \"day\", \"hour\")),\n         y = 1000)\n\npp_plot <- pp_tidy %>% \n  mutate(model = factor(model, levels = c(\"week\", \"day\", \"hour\"))) %>%\n  ggplot() + \n  geom_histogram(aes(x = posterior, fill = model), color = \"black\", alpha = .4, \n                 bins = 30) +\n  geom_segment(mapping = aes(y = y+100, yend = y-100, x = mean, xend = mean,\n                           color = model),\n               show.legend = FALSE,\n               data = ci) +\n  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),\n              show.legend = FALSE,\n               data = ci) +\n  geom_text(data = ci, x = c(.93, .907, .92), y = 1000, \n            label = str_c(round(ci$mean, 2), \" [\", round(ci$lower, 2), \", \", round(ci$upper, 2), \"]\")) +\n  facet_wrap(~model, ncol = 1) +\n  scale_y_continuous(\"Posterior Probability\", breaks = c(0, 500, 1000)) +\n  xlab(\"Area Under ROC Curve\") +\n  theme(strip.background = element_blank(),\n        strip.text.x = element_blank(),\n        legend.position = \"bottom\")\n\nplot_grid(roc_plot, pp_plot, ncol = 2, rel_widths = c(1, 1.25))\n\n\n\n\nInsert note here\n\n\n\n\n\n\nfig_caption_roc <- \"Receiver Operating Characteritic Curves by Model  \"\n\n\nroc_all %>% \n  mutate(model = factor(model, levels = c(\"1week\", \"1day\", \"1hour\"), \n                        labels = c(\"week\", \"day\", \"hour\"))) %>% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  labs(x = \"Specificity\",\n       y = \"Sensitivity\") +\n  scale_x_continuous(breaks = seq(0,1,.25),\n    labels = sprintf(\"%.2f\", seq(1,0,-.25)))\n\n\n\n\nReceiver Operating Characteritic Curves by Model\n\n\n\n\n\n\nfig_caption_pp <- \"Posterior Probability Distributions for Area Under the ROC Curve by Model.  Horizontal lines represent 95 percent credible intervals for each model.  Verical line represents mean of the posteerior distribution.\"\n\n\npp_tidy <- pp %>% \n  tidy(seed = 123)\n\nci <- pp_tidy %>% \n  summary() %>% \n  mutate(model = factor(model, levels = c(\"week\", \"day\", \"hour\")),\n         y = 1000)\n\npp_tidy %>% \n  mutate(model = factor(model, levels = c(\"week\", \"day\", \"hour\"))) %>%\n  ggplot() + \n  geom_histogram(aes(x = posterior, fill = model), color = \"black\", alpha = .4, \n                 bins = 30) +\n  geom_segment(mapping = aes(y = y+100, yend = y-100, x = mean, xend = mean,\n                           color = model),\n               show.legend = FALSE,\n               data = ci) +\n  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),\n              show.legend = FALSE,\n               data = ci) +\n  geom_text(data = ci, x = c(.925, .912, .915), y = 1000, \n            label = str_c(round(ci$mean, 2), \" [\", round(ci$lower, 2), \", \", round(ci$upper, 2), \"]\")) +\n  facet_wrap(~model, ncol = 1) +\n  scale_y_continuous(\"Posterior Probability\", breaks = c(0, 500, 1000)) +\n  xlab(\"Area Under ROC Curve\") \n\n\n\n\nPosterior Probability Distributions for Area Under the ROC Curve by Model. Horizontal lines represent 95 percent credible intervals for each model. Verical line represents mean of the posteerior distribution.\n\n\n\n\n\n\n\npr_.75_cutoff <- pr_all %>% \n  mutate(recall = round(recall, 3),\n         precision = round(precision, 3),\n         .threshold = round(.threshold, 3),\n         model = factor(model,\n                        levels = c(\"1week\", \"1day\", \"1hour\"),\n                        labels = c(\"week\", \"day\", \"hour\"))) %>% \n  filter(precision == .75) %>% \n  group_by(model, precision) %>% \n  summarise(recall = mean(recall),\n            threshold = mean(.threshold),\n            .groups = \"drop\")\n\n\n\n\n\nfig_caption_pr <- \"Precision-Recall Curves for models.\"\n\n\npr_all %>% \n  mutate(model = factor(model, levels = c(\"1week\", \"1day\", \"1hour\"),\n                        labels = c(\"week\", \"day\", \"hour\"))) %>%\n  ggplot(aes(x = recall, y = precision, color = model)) +\n  geom_path() +\n  geom_segment(mapping = aes(y = .75, yend = .75, x = -.5, xend = recall,\n                           color = model),\n               linetype = \"dashed\",\n               alpha = .8,\n               show.legend = FALSE,\n               data = pr_.75_cutoff) +\n  geom_segment(mapping = aes(y = -.5, yend = .75, x = recall, xend = recall,\n                           color = model),\n               linetype = \"dashed\",\n               alpha = .8,\n               show.legend = FALSE,\n               data = pr_.75_cutoff) +\n  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n  labs(x = \"Sensitivity (Recall)\",\n       y = \"Positive Predictive Value (Precision)\")\n\n\n\n\nPrecision-Recall Curves for models.\n\n\n\n\n\n\nfig_caption_shapgrouped <- \"Variable Importance (SHAP Values) for each Model.  Raw EMA features are grouped by the original item from the EMA. Features from demographics and the day and hour for the start of the  prediction window are also included.\"\n\n\nshap_grouped_all <- shap_grouped_week %>% \n  mutate(window = \"week\") %>% \n  bind_rows(shap_grouped_day %>% \n              mutate(window = \"day\")) %>% \n  bind_rows(shap_grouped_hour %>% \n              mutate(window = \"hour\")) %>% \n  mutate(window = factor(window, levels = c(\"week\", \"day\", \"hour\"))) %>% \n  mutate(group = factor(group, levels = c(\"past use (EMA item)\", \n                                          \"craving (EMA item)\", \n                                          \"past risky situation (EMA item)\", \n                                          \"past stressful event (EMA item)\", \n                                          \"past pleasant event (EMA item)\", \n                                          \"valence (EMA item)\", \n                                          \"arousal (EMA item)\", \n                                          \"future risky situation (EMA item)\", \n                                          \"future stressful event (EMA item)\", \n                                          \"future efficacy (EMA item)\",\n                                          \"lapse day (other)\",\n                                          \"lapse hour (other)\",\n                                          \"missing surveys (other)\",\n                                          \"age (demographic)\",\n                                          \"sex (demographic)\",\n                                          \"race (demographic)\",\n                                          \"marital (demographic)\",\n                                          \"education (demographic)\")))\n\nshap_grouped_all %>% \n  mutate(group = reorder(group, mean_value, sum)) %>% \n  ggplot() +\n  geom_bar(aes(x = group, y = mean_value, fill = window), stat = \"identity\", alpha = .4) +\n  ylab(\"Mean |SHAP| value\") +\n  xlab(\"\") +\n  coord_flip()\n\n\n\n\nVariable Importance (SHAP Values) for each Model. Raw EMA features are grouped by the original item from the EMA. Features from demographics and the day and hour for the start of the prediction window are also included."
  }
]