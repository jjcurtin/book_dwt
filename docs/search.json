[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data Wrangling in the Tidyverse",
    "section": "Welcome",
    "text": "Welcome\nJohn Curtin and his students and staff at the Addiction Research Center are developing this book to document what we believe to be best practices (or at least our practices) for data wrangling and related tasks using the Tidyverse and Tidymodels ecosystems and Quarto publishing system.\nThis book is under active development with its most recent edits on 2023-05-02. We hope that this resource will continue to evolve to serve both our lab and the R community more generally."
  },
  {
    "objectID": "conflicts.html#minmize-loading-of-full-packages",
    "href": "conflicts.html#minmize-loading-of-full-packages",
    "title": "1  Function conflicts",
    "section": "1.1 Minmize loading of full packages",
    "text": "1.1 Minmize loading of full packages\nFunction conflicts can be minimized by limiting the number of packages that you attach in your scripts. For our work, we will almost always use library(tidyverse) and frequently use library(tidymodels) next. You should carefully consider if you need to attach any other packages. You very well may not!\nYou should only attach full packages if you intend to use multiple functions from that package. If instead, you only need a single function (or several), there are two alternatives that are preferred over attaching the full package with library().\nOption 1: Use the namespace of the function when calling it in your script. For example, if I need to simulate multivariate normal data, I might want to use the mvrnorm() function from the MASS package. I do NOT need to use library(MASS) to use this function. Instead, I can simply call the function with its namespace MASS::mvrnorm(). This will avoid conflicts between other functions in MASS and your other attached packages (e.g., select() in MASS conflicts with select() in dplyr/tidyverse).\nOption 2: If you find using the namespace of the function cumbersome, you can attach a single function from a package rather than the full package. For example, if we wanted to use only mvrnorm() from MASS, we could use this code: library(MASS, include.only = \"mvrnorm). Now you can call mvrnorm() without pre-pending its namespace (MASS::). You can pass a character vector containing multiple function names rather than a single function to include.only if you intend to use several functions from the package (e.g., library(MASS, include.only = c(\"mvrnorm\", \"lda\")))."
  },
  {
    "objectID": "conflicts.html#base-r-conflict-managemnt",
    "href": "conflicts.html#base-r-conflict-managemnt",
    "title": "1  Function conflicts",
    "section": "1.2 Base R conflict managemnt",
    "text": "1.2 Base R conflict managemnt\nAs of version 3.6, R now includes all the necessary tools (in our opinion) to handle and clearly resolve function conflicts. These tools are well-documented and should be reviewed to better understand how to use them.\nFor our purposes, it is generally sufficient to use one of the two named conflict policies that are included (depends.ok or strict). We prefer the use of the depends.ok policy.\nTo implement the depends.ok policy, simply set this option at the top of your script using options(conflicts.policy = \"depends.ok\"). You can now combine this option with limited use of library() for important packages and the use of either namespace or include.only methods described above and you should be good to go (with one exception noted below).\nTo get a better sense of what the depends.ok policy does, it is a shortcut to implement the following set of conflict options.\n\noptions(conflicts.policy =\n            list(error = TRUE,\n                 generics.ok = TRUE,\n                 can.mask = c(\"base\", \"methods\", \"utils\",\n                              \"grDevices\", \"graphics\",\n                              \"stats\"),\n                 depends.ok = TRUE))\n\nThis means that packages that you attach with library will produce an error if their functions conflict with previously loaded packages (error = TRUE).\nHowever, errors will not occur if the functions conflict with functions in base R (i.e., base R packages are listed in can.mask =) or S4 generic versions (generics.ok = TRUE). These exceptions generally make sense because package functions are often explicitly intended to mask or extend these functions.\nAn error will also not be produced if function conflicts exist within a single package (depends.ok = TRUE) because the package creator typically intended this as well.\nErrors due to other function conflicts will happen immediately when you try to load the new package so you can address these conflicts up front (by either including only a subset of functions from the library or using the function’s namespace instead).\nThere are more advanced tools to handle conflicts in special cases that are also described in the documentation but they should rarely be necessary.\nThere is one more detail that affects our common use of both tidyverse and tidymodels collections of packages. The depends.ok policy allows for conflicts/masking within a package. However, there are a couple of instances where some of the functions in tidymodels will conflict/mask functions in packages within tidyverse. R doesn’t recognize these as the same package so the conflicts need to be handled explicitly. You can do this using conflictRules() Here are the conflict rules you will need to attach tidymodels after tidyverse\n\nconflictRules(\"scales\", mask.ok = c(\"discard\", \"col_factor\"))\nconflictRules(\"recipes\", mask.ok = c(\"fixed\"))\nconflictRules(\"yardstick\", mask.ok = c(\"spec\"))\n\nHowever, given how often we do this, we have wrapped these rules into a simple function. This function is reproduced here, but also shared in our fun_ml.R script in lab_support on github. If you source that script, you can call this function directly along with setting the depends.ok policy.\n\ntidymodels_conflictRules <- function(){\n  conflictRules(\"scales\", mask.ok = c(\"discard\", \"col_factor\"))\n  conflictRules(\"recipes\", mask.ok = c(\"fixed\"))\n  conflictRules(\"yardstick\", mask.ok = c(\"spec\"))\n}"
  },
  {
    "objectID": "conflicts.html#a-short-example",
    "href": "conflicts.html#a-short-example",
    "title": "1  Function conflicts",
    "section": "1.3 A Short Example",
    "text": "1.3 A Short Example\nTo implement the depends.ok policy and the rules for tidymodels, put this code chunk at the top of your script, prior to attaching any packages\n\noptions(conflicts.policy = \"depends.ok\")\ntidymodels_conflictRules()\n\nNow you can attach your packages and resolve any errors\n\nLoad tidyverse first and then load tidymodels, allowing it to make functions in packages within tidyverse as indicated below\nWe demonstrate loading only clean_names() from the janitor package\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor, include.only = \"clean_names\")\n\nAnd if you later decided you wanted to simulate data you could use the appropriate MASS function directly with its namespace. Notice use of mvrnorm() with namespace because we did not attach the MASS package. Notice use of clean_names() without namespace because we attached just that function for janitor package.\n\nd <- MASS::mvrnorm(n = 10, mu = c(0,0), Sigma = diag(2)) %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  clean_names(\"snake\")"
  },
  {
    "objectID": "conflicts.html#conflicted-packakge",
    "href": "conflicts.html#conflicted-packakge",
    "title": "1  Function conflicts",
    "section": "1.4 conflicted packakge",
    "text": "1.4 conflicted packakge\nYou should be aware that an alternative solution to handling function conflicts is provided in the conflicted package. However, this is not our preferred solution as the base R conflict policies are sufficient (so why load another package!). We also prefer to have conflicts detected immediately (when packages are attached) rather than at some later point when we call the function. The conflicted package is also less customizable that the base R polices."
  },
  {
    "objectID": "cache.html#solutions",
    "href": "cache.html#solutions",
    "title": "2  Cache",
    "section": "2.1 Solutions",
    "text": "2.1 Solutions\n\n2.1.1 cache = TRUE (not recommended)\nYou can set cache = TRUE in any specific code chunk to have knitr cache those calculations for your later reuse. However, we don’t recommend this because it makes the process and instances where the cache is invalidated more opaque. And more importantly, this caching will not be used for interactive use when you send your code chunks to the console as you work live.\nNonetheless, this approach is well documented including more advanced topics like paths and lazy loading.\n\n\n2.1.2 Explicit write_rds() (not recommended\nYou could instead manually save objects that you want to avoid recalculating. This is a legitimate method that gives you full and transparent control over caching. It will also work both interactively in the console and when you knit your document. However, its got a bit more overhead RE the code. You need to write code to check if the file exists and load it if it does vs. calculate the object if it doesn’t. This is not too hard but it turns out that a function has already been written to handle this overhead for you. We describe that next.\n\n\n2.1.3 xfun::cache_rds() (our preferred method)\nWe believe that the xfun::cache_rds() function provides the sweet spot for the balance of control and transparency vs. code overhead. It also works for both interactive/console and knit workflows.\nLets demonstrate its use. We start by setting up some objects that will be used in later time-consuming calculations. You need to be careful with these objects. If they ever change, you will need to explicitly invalidate your cached object and re-calculate it. More on that below.\n\ny <- 2\nz <- 3\n\nNow lets use y and z in some time consuming set of calculations\n\nThe first argument parameter in cache_rds() is the code to execute the time-consuming calculation. This code is provided to the function inside of curly brackets, {}\nResults from cache_rds() are assigned to your object (e.g., x) as if they came straight from the coded calculations (e.g., instead of x <- y + z, we now have x <- cache_rds({y + z})).\nWe recommend explicitly setting the values for the dir and file for the cached object. This way, you control where it is saved and are assured it will be the same location regardless of whether you run this code chunk in the console or knit it. Initial testing suggested the filename and location will differ for interactive/console vs. knit workflows if you use defaults. The / at the end of the directory name is required to designate this as a folder. The filename will have the string assigned to file as the prefix but will have an additional hash and a .rds appended to it as well.\nWe recommend explicitly including rerun = FALSE as a third parameter. This provides you an easy way to invalidate the cached object (and a memory aid to consider invalidation when needed). To invalidate, just set it to TRUE and run the chunk again again if any of your globals (e.g., y, z) have changed (and then set back to FALSE after!). Alternatively, you can invalidate the cached object by deleting it from the cache/ folder.\ncache_rds() has one additional parameter worth mentioning, hash. You can pass a list of global objects to hash (e.g. hash = list(y, z)) that the function will monitor for change. If any of these globals are re-calculated, it will invalidate your cached object and re-calculate it. This can be very useful and we should start using it too. However, our testing suggests that it may invalidate the cache in some instances when it shouldn’t. We havent been able to fully document this issue yet. For now, we recommend using this and paying attention to when the cache is incorrectly invalidated (if ever). Some testing has also suggested that you may need to use >= R version 4.3 to avoid the problem with cache invalidation (but that isn’t definite either :-( We will update this page as we better understand if this is a persistent issue.\n\n\nx <- xfun::cache_rds({\n  Sys.sleep(5) # pretend that computations take a while\n  y + z\n},\nrerun = FALSE,\nhash = list(y, z),\ndir = \"cache/\",\nfile = \"cache_demo\")\n\nNow we can use x without recalculating it each time when executing the previous chunk in either console or when knit. Yay!\n\nx\n\n[1] 5\n\n\nYou can (and should) read the full documentation on xfun::cache_rds() prior to using it in your own code.\n\n\n2.1.4 Final notes\n\n2.1.4.1 Cache and Github\nWe obviously don’t want our cached objects getting added to our repos. This could make the repos become too large and/or add sensitive data to them. It is easy to avoid this though. Assuming you are calling the folder that you will save the cached files to cache/ as recommended above, just add the following line to your .gitignore file\n*_cache/\n\n\n2.1.4.2 Saving model objects\nWe have also learned that caching that involves saving an rds file (all of these methods) may encounter problems if you try to cache a keras model object (e.g., via mlp() in tidymodels). To be clear, there is no problem saving resampling statistics from fit_resamples() or tune_grid(). The problem is specific to the actual model object returned from fit(). This issue with keras (and perhaps some other types) models is documented and the bundles package is designed to solve it. If you plan to cache (or even just directly save) these model objects, read these docs carefully. We will eventually work out a piped solution that works to either manually save or use cache_rds() with these objects if needed. Not a high priority for us right now because we dont use keras much yet in our lab."
  },
  {
    "objectID": "parallel_processing.html#tune_grid-in-tidymodels",
    "href": "parallel_processing.html#tune_grid-in-tidymodels",
    "title": "3  Parallel Processing",
    "section": "3.1 tune_grid() in tidymodels",
    "text": "3.1 tune_grid() in tidymodels\nSet up data, resamples, recipe, tuning grid. Will do 3x 10-fold CV to tune an elasticnet glm with a sample size of 1000 and 30 features\n\n# set up data\nn_obs <- 1000\nn_x <- 30\nirr_err <- 5\nd <- MASS::mvrnorm(n = n_obs, mu = rep(0,n_x), Sigma = diag(n_x)) %>% \n    magrittr::set_colnames(str_c(\"x\", 1:n_x)) %>% \n    as_tibble() %>% \n    mutate(error = rnorm(n_obs, 0, irr_err),\n           y = rowSums(across(everything()))) %>% \n    select(-error)\n\n# recipe\nrec <- recipe(y ~ ., data = d)\n\n# 3x 10-fold CV\nset.seed(19690127)\nsplits <- d %>% \n  vfold_cv(v = 10, repeats = 3, strata = \"y\")\n\n# tuning grid\ntune_grid <- expand_grid(penalty = exp(seq(0, 6, length.out = 200)),\n                           mixture = seq(0, 1, length.out = 11))\n\nFirst, let’s benchmark without parallel processing. tune_grid() (and fit_resamples()) default is to allow parallel processing so have to explicitly turn it off using control_grid(). You will NOT do this. It is only to show the benefits of parallel processing.\n\ntic()\nlinear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\") %>% \n  tune_grid(preprocessor = rec, \n            resamples = splits, grid = tune_grid, \n            metrics = metric_set(rmse),\n            control = control_grid(allow_par = FALSE)) # turn off pp\n\n# Tuning results\n# 10-fold cross-validation repeated 3 times using stratification \n# A tibble: 30 × 5\n   splits            id      id2    .metrics             .notes          \n   <list>            <chr>   <chr>  <list>               <list>          \n 1 <split [900/100]> Repeat1 Fold01 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 2 <split [900/100]> Repeat1 Fold02 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 3 <split [900/100]> Repeat1 Fold03 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 4 <split [900/100]> Repeat1 Fold04 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 5 <split [900/100]> Repeat1 Fold05 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 6 <split [900/100]> Repeat1 Fold06 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 7 <split [900/100]> Repeat1 Fold07 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 8 <split [900/100]> Repeat1 Fold08 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 9 <split [900/100]> Repeat1 Fold09 <tibble [2,200 × 6]> <tibble [0 × 3]>\n10 <split [900/100]> Repeat1 Fold10 <tibble [2,200 × 6]> <tibble [0 × 3]>\n# ℹ 20 more rows\n\ntoc()\n\n48.08 sec elapsed\n\n\nNow allow use of parallel processing (the default). No plan is needed here (consistent with findings for foreach()). Yay!\n\ntic()\nlinear_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\") %>% \n  tune_grid(preprocessor = rec, \n            resamples = splits, grid = tune_grid, \n            metrics = metric_set(rmse))\n\n# Tuning results\n# 10-fold cross-validation repeated 3 times using stratification \n# A tibble: 30 × 5\n   splits            id      id2    .metrics             .notes          \n   <list>            <chr>   <chr>  <list>               <list>          \n 1 <split [900/100]> Repeat1 Fold01 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 2 <split [900/100]> Repeat1 Fold02 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 3 <split [900/100]> Repeat1 Fold03 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 4 <split [900/100]> Repeat1 Fold04 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 5 <split [900/100]> Repeat1 Fold05 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 6 <split [900/100]> Repeat1 Fold06 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 7 <split [900/100]> Repeat1 Fold07 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 8 <split [900/100]> Repeat1 Fold08 <tibble [2,200 × 6]> <tibble [0 × 3]>\n 9 <split [900/100]> Repeat1 Fold09 <tibble [2,200 × 6]> <tibble [0 × 3]>\n10 <split [900/100]> Repeat1 Fold10 <tibble [2,200 × 6]> <tibble [0 × 3]>\n# ℹ 20 more rows\n\ntoc()\n\n30.91 sec elapsed"
  },
  {
    "objectID": "parallel_processing.html#final-notes",
    "href": "parallel_processing.html#final-notes",
    "title": "3  Parallel Processing",
    "section": "3.2 Final notes",
    "text": "3.2 Final notes\nThe following is often found as an alternative setup for a back-end for parallel processing. It works for future_map() (when conbined with plan) and for foreach() but not in the tidymodels implementations of resampling. Not clear why since those use foreach() but this should not be used if you plan to use tidymodels resampling.\n\nlibrary(doFuture)\nregisterDoFuture()\n\nI tried this with both directly and with various options of plan()\n\n# plan(multisession, workers = parallel::detectCores(logical = FALSE))\n\nand with\n\n# cl <- makeCluster(parallel::detectCores(logical = FALSE))\n# plan(cluster, workers = cl)"
  },
  {
    "objectID": "file_and_path_management.html#use-of-rstudio-projects",
    "href": "file_and_path_management.html#use-of-rstudio-projects",
    "title": "4  Best Practices for File and Path Management",
    "section": "4.1 Use of RStudio Projects",
    "text": "4.1 Use of RStudio Projects\nThe use of RStudio Projects is critical to good managament of your paths and files. When you work within a project, you will have a working directory set within that project (based on where the project files is saved. This working directory can then be combined with relative paths for reading and writing data and other files. It also means that if you share the folders that contain your project (e.g., scripts, data), the paths will continue to work for that colleague as well, regardless of where they situate the folders on their computer.\nWickham et al., describe the rationale and benefits for using projects. Please read this! They also clearly describe the steps to set up a new project so I won’t repeat them here.\nFor our course, we strongly recommend that you set up a project called “iaml”. Inside that root project folder, you can establish a folder for “homework”, and inside that folder you can have sub-folders for each unit (e.g., “unit_2”, “unit_3”). In addition to the homework folder, you might have folders for exams (e.g., “midterm”) and other material that you save (e.g., “pdfs”)."
  },
  {
    "objectID": "file_and_path_management.html#relative-paths",
    "href": "file_and_path_management.html#relative-paths",
    "title": "4  Best Practices for File and Path Management",
    "section": "4.2 Relative Paths",
    "text": "4.2 Relative Paths\nYou should also get in the habit of setting relative paths (relative to your project root) near the start of your script so that you can call those paths easily throughout. Added bonus, if you move those folders within your project, you just need to change one line of code. For example if your raw data and processed data live in separate folders you might have two paths set:\npath_raw <- \"data/raw\"\npath_processed <- \"data/processed\"\nYou can use these path objects with the here() function, which is described next."
  },
  {
    "objectID": "file_and_path_management.html#use-of-here",
    "href": "file_and_path_management.html#use-of-here",
    "title": "4  Best Practices for File and Path Management",
    "section": "4.3 Use of here()",
    "text": "4.3 Use of here()\nhere() in the package here is a replacement for file.path() that we find to work comparable (and better in a few specific circumstances; e.g., when knitting Rmd files). It is very easy to use if you are using Projects as recommended above. That said, it can also be used with the use of Projects, but we dont review that here because you WILL use projects! To use here(), you should load the here package at the top of your script (i.e., library(here)).\nYou can then use here() as a replacement anywhere you would have used file.path()\nFor example, if you want to load a csv file in the folder that you indicated above by path_raw, you could use this line of code:\nd <- read_csv(here(path_raw, \"raw_data.csv\"))\nalternatively, you could supply the relative path directly (though this is not preferred because it can be cumbersome if you move the folder later)\nd <- read_csv(here(\"data/processed\", \"raw_data.csv\"))"
  },
  {
    "objectID": "file_and_path_management.html#sourcing-from-github",
    "href": "file_and_path_management.html#sourcing-from-github",
    "title": "4  Best Practices for File and Path Management",
    "section": "4.4 Sourcing from Github",
    "text": "4.4 Sourcing from Github\nScripts in public repositories on GithHub can be sourced directly from the remote repository on GitHub using source_url() from the `devtools’ package. To do this, follow these steps:\n\nFind the url to the specific file/script you would like to source. This can be done by simply clinical on the file through GitHub in your browser. For example, the url to fun_modeling.R in my lab_support repo is:\n\nhttps://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R\n\nAdd ?raw=true to the end of that url. For example:\n\nhttps://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\n\nPass this url as a string into devtools::source_url()` in your R script. For example:\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")\nIts that easy. Using this method will allow you to continue to use the most up-to-date version of that script even as the repo owner improves it over time. It also doesnt require you to worry about where a local clone of that repo might live on your computer or the computers of anyone with which you share your code."
  },
  {
    "objectID": "file_and_path_management.html#additional-resources",
    "href": "file_and_path_management.html#additional-resources",
    "title": "4  Best Practices for File and Path Management",
    "section": "4.5 Additional Resources",
    "text": "4.5 Additional Resources\n\nBlog with links on the use of projects and here() package\nGood advice for folder management in projects.\nMore good advice on projects and file management"
  },
  {
    "objectID": "iteration.html",
    "href": "iteration.html",
    "title": "5  Iteration",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\ntidymodels_prefer()"
  },
  {
    "objectID": "reproducible_examples.html#why-create-a-reprex",
    "href": "reproducible_examples.html#why-create-a-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.1 Why create a reprex?",
    "text": "6.1 Why create a reprex?\nCareful use of a reproducible example (a reprex) is critical for…\n\nDebugging your code\nAsking for help on listservs (e.g., Posit/Rstudio community, StackOverflow, and others)\nPosting bug reports for tidymodels or tidyverse packages on GitHub\n\nA reproducible example should be the simplest snippet of code that can reproduce the error that you are encountering. By creating this simple example, it will focus everyone on the nature of your problem(s), reproduce the error message (or incorrect output), and not distract by including code that is irrelevant to your problem."
  },
  {
    "objectID": "reproducible_examples.html#key-featrures-of-a-good-reprex",
    "href": "reproducible_examples.html#key-featrures-of-a-good-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.2 Key Featrures of a Good Reprex",
    "text": "6.2 Key Featrures of a Good Reprex\nThere are a few key features of a good reprex.\n\nThe reprex should run completely on its own with just copy/paste. It should only require the snippet of shared code to run without any additional scripts, data, etc. This makes it easy for everyone to copy your code into their IDE and run it themselves. This is critical if you want someone else to use their time to help you solve YOUR problem.\nIt should NOT require anyone to source other scripts If you used sourced functions, copy them into the reprex. Or better yet, don’t require those functions at all unless they are the source of the error.\nIt should (almost) never require the person to load data. You should either use fake data (i.e., simulated data) that you create at the start of the reprex or data that you load from the modeldata package (i.e., include library(modeldata) at the top of the reprex).\nRemove all extra lines of code from the reprex that are not necessary to either set up the data or produce the error. Your goal is to use the minimum lines of code needed to reproduce the error.\nInclude only necessary packages. Unfortunately, package conflicts are common in R. By stripping out unnecessary packages from your reprex, you may find the source of a package conflict."
  },
  {
    "objectID": "reproducible_examples.html#debugging-with-your-reprex",
    "href": "reproducible_examples.html#debugging-with-your-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.3 Debugging with your Reprex",
    "text": "6.3 Debugging with your Reprex\nYou will find that in the process of simplifying your code, you will find the source of your error yourself in many, many instances. This process of cutting extraneous code and simplifying it is the recommended process for debugging your code systematically.\nOnce you have your simple reprex, you can now experiment with it to find the boundary conditions for the error.\n* Read help on the function that seems to generate the error.\n* Check the name of your variables, the function, and the function parameters.\n* Make sure you are assigning the correct argument to each function parameter. You may want to name the parameters formally rather than relying on the order they are listed in the function).\n* Use R debugging tools like traceback and others.\n* Look carefully at your tibble. Consider the class of all the variables. Are missing values causing an issue?"
  },
  {
    "objectID": "reproducible_examples.html#using-reprex-package-to-post-your-reprex",
    "href": "reproducible_examples.html#using-reprex-package-to-post-your-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.4 Using reprex Package to Post your Reprex",
    "text": "6.4 Using reprex Package to Post your Reprex\nIf you cannot solve the problem after doing substantial testing and debugging within your reprex on your own, it is time to ask for help. In the course, you will ask for help from us (John and the TAs). In the future, you will ask for help from the R community (see options described above). In either instance, be respectful and only ask after you have tried hard to solve the problem on your own and after you’ve truly hit a wall.\nIf you are asking for help, you will need to describe your problem and post your reprex. Try to describe the nature of the issue clearly (though in some instances all that takes is reporting the error message). It might take more description if the problem is not a formal error in the code but incorrect output. The post the reprex. A provides a package (reprex) to help you create markdown code to post your reprex. Here are the steps to use it.\n\nMake sure you have installed the reprex package (install.packages(\"reprex\"); you do not need to load it, only install it).\nCopy our reprex code to your clipboard.\ntype reprex::reprex(venue = \"slack\") (substitute “gh” for slack if you are posting to github, the rstudio/posit community, or StackOverflow).\nAfter the reprex function runs, it will put markdown code onto your clipboard. You can now go back to your post and paste it in. Its that simple!"
  },
  {
    "objectID": "reproducible_examples.html#using-data-in-your-reprex",
    "href": "reproducible_examples.html#using-data-in-your-reprex",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.5 Using Data in your Reprex",
    "text": "6.5 Using Data in your Reprex\nYou should (almost) never need to provide a separate data file with your reprex (and it just makes things more complicated for those trying to help you if you do).\nInstead, you can often simulate a simple data set. Here is a dataset with variables for different classes including numeric, character and factor.\n\nlibrary(dplyr)\nset.seed(12345) # to make data reproducible if that matters\nd <- tibble (x = rnorm(24), \n             y = rep(c(\"group_1\", \"group_2\"), 12), \n             z = factor(rep(c(\"level_1\", \"level_2\", \"level_3\"), 8))) %>% \n  glimpse()\n\nRows: 24\nColumns: 3\n$ x <dbl> 0.5855288, 0.7094660, -0.1093033, -0.4534972, 0.6058875, -1.8179560,…\n$ y <chr> \"group_1\", \"group_2\", \"group_1\", \"group_2\", \"group_1\", \"group_2\", \"g…\n$ z <fct> level_1, level_2, level_3, level_1, level_2, level_3, level_1, level…\n\nd %>% print()\n\n# A tibble: 24 × 3\n        x y       z      \n    <dbl> <chr>   <fct>  \n 1  0.586 group_1 level_1\n 2  0.709 group_2 level_2\n 3 -0.109 group_1 level_3\n 4 -0.453 group_2 level_1\n 5  0.606 group_1 level_2\n 6 -1.82  group_2 level_3\n 7  0.630 group_1 level_1\n 8 -0.276 group_2 level_2\n 9 -0.284 group_1 level_3\n10 -0.919 group_2 level_1\n# ℹ 14 more rows\n\n\nThis should get you started with some edits to fit your scenario. For other situations, the functions caret::twoClassSim() or caret::SLC14_1() might be good tools to simulate data for you.\nAlternatively, you can include library(modeldata) at the top of your reprex and then have access to all the datasets in that package. You can see a list of the dataset names for the repo on Github.\names might be a good starting point for you for fake data because it has lots of options for variables. I find the variable names a bit annoying (because they are not snake_case) but that shouldn’t be an issue for a simple reprex. You should probably select down to just the variables you need and slice out a random sample of fewer cases for use in your reprex. For example…\n\nlibrary(modeldata)\names_1 <- ames %>% \n  select(Sale_Price, Lot_Config, Lot_Frontage) %>% \n  slice_sample(n = 50) %>% \n  glimpse()\n\nRows: 50\nColumns: 3\n$ Sale_Price   <int> 146000, 755000, 115000, 160000, 133000, 187000, 285000, 1…\n$ Lot_Config   <fct> Inside, Corner, Inside, Corner, Inside, Inside, Inside, C…\n$ Lot_Frontage <dbl> 24, 104, 100, 87, 34, 65, 0, 0, 0, 70, 160, 66, 50, 59, 5…"
  },
  {
    "objectID": "reproducible_examples.html#more-help",
    "href": "reproducible_examples.html#more-help",
    "title": "6  How to Create a Reproducible Example (Reprex)",
    "section": "6.6 More Help",
    "text": "6.6 More Help\nFor more help see:\n\nHow to use a Reprex from tidyverse folks\n[What is a reprex] (https://github.com/tidyverse/reprex#what-is-a-reprex) from tidyverse folks.\nHow to make a great R reproducible example from Stack Overflow *How to create a reprex from Stack Overflow\nA good example bug report from tidymodels on Github: #46"
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "7  Simulations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\ntidymodels_prefer()"
  },
  {
    "objectID": "programming.html#reading-csv-files",
    "href": "programming.html#reading-csv-files",
    "title": "8  Programming Notes",
    "section": "8.1 Reading csv files",
    "text": "8.1 Reading csv files\nWe typically save our data as csv files. There are many benefits to this format (e.g., easy to share, easy to view outside of R) but one downside is that the don’t store information about variable/column class. We need to establish the appropriate class for each column when we read the data.\n\n8.1.1 Using col_types()\nIf possible, it is best to set the class for each column/variable specifically using the col_types() parameter in read_csv() This forces you to specifically examine and consider each column to decide its class (e.g., is a column with numbers best set as numeric or ordered factor) and the levels if its class is nominal. Of course, this is part of cleaning EDA so you should have done this when you first started working with the data.\nRe-classing is typically needed to convert raw character columns to factor (ordered or unordered) and sometimes to convert raw numeric columns to factor (likely ordered, e.g., a likert scale).\nHere is an example using the cars dataset\n\ndf <- read_csv(here(path_data, \"auto_trn.csv\"),\n               col_type = list(mpg = col_factor(levels = c(\"low\", \"high\")),\n                               # here we handle cylinders as an ordered factor\n                               cylinders = col_factor(levels = \n                                                        as.character(c(3,4,5,6,8)), \n                                                      ordered = TRUE),   \n                               displacement = col_double(),\n                               horsepower = col_double(),\n                               weight = col_double(),\n                               acceleration = col_double(),\n                               year = col_double(),\n                               origin = col_factor(levels = \n                                                     c(\"american\", \n                                                       \"japanese\", \n                                                       \"european\")))) %>% \n  glimpse()\n\nRows: 294\nColumns: 8\n$ mpg          <fct> high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    <ord> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement <dbl> 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   <dbl> 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       <dbl> 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration <dbl> 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       <fct> japanese, japanese, european, european, european, europea…\n\n\n\n\n8.1.2 Using a separate mutate()\nIn some instances (e.g., data file with very large number of variables, very consistently organized data character data is well-behaved), you may want to read the data in first and then use mutate() to change classes as needed.\nIn these instances, we prefer to set the col_types() parameter to cols() to prevent the verbose message about column classes.\nHere is an example using the ames dataset with all predictors. To start, we only re-class all character columns to unordered factor and one numeric column to an ordered factor. As we work with the data (during cleaning EDA), we may decide that there are other columns that need to be re-classed. If so, we could add additional lines to the mutuate()\n\npath_unit6 <- \"homework/unit_6\"\ndf <- read_csv(here(path_data, \"ames_full_cln.csv\"),\n               col_types = cols()) %>% \n  # convert all character to unordered factors\n  mutate(across(where(is.character), as_factor),\n         overall_qual = ordered(overall_qual, levels = as.character(1:10))) %>% \n  glimpse()\n\nRows: 1,955\nColumns: 81\n$ pid             <fct> x0526301100, x0526350040, x0526351010, x0527105010, x0…\n$ ms_sub_class    <fct> x020, x020, x020, x060, x120, x120, x120, x060, x060, …\n$ ms_zoning       <fct> rl, rh, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl…\n$ lot_frontage    <dbl> 141, 80, 81, 74, 41, 43, 39, 60, 75, 63, 85, NA, 47, 1…\n$ lot_area        <dbl> 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10…\n$ street          <fct> pave, pave, pave, pave, pave, pave, pave, pave, pave, …\n$ alley           <fct> none, none, none, none, none, none, none, none, none, …\n$ lot_shape       <fct> ir1, reg, ir1, ir1, reg, ir1, ir1, reg, ir1, ir1, reg,…\n$ land_contour    <fct> lvl, lvl, lvl, lvl, lvl, hls, lvl, lvl, lvl, lvl, lvl,…\n$ utilities       <fct> all_pub, all_pub, all_pub, all_pub, all_pub, all_pub, …\n$ lot_config      <fct> corner, inside, corner, inside, inside, inside, inside…\n$ land_slope      <fct> gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl, gtl,…\n$ neighborhood    <fct> n_ames, n_ames, n_ames, gilbert, stone_br, stone_br, s…\n$ condition_1     <fct> norm, feedr, norm, norm, norm, norm, norm, norm, norm,…\n$ condition_2     <fct> norm, norm, norm, norm, norm, norm, norm, norm, norm, …\n$ bldg_type       <fct> one_fam, one_fam, one_fam, one_fam, twhs_ext, twhs_ext…\n$ house_style     <fct> x1story, x1story, x1story, x2story, x1story, x1story, …\n$ overall_qual    <ord> 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, …\n$ overall_cond    <dbl> 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 2, 5, 6, 6, …\n$ year_built      <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, …\n$ year_remod_add  <dbl> 1960, 1961, 1958, 1998, 2001, 1992, 1996, 1999, 1994, …\n$ roof_style      <fct> hip, gable, hip, gable, gable, gable, gable, gable, ga…\n$ roof_matl       <fct> comp_shg, comp_shg, comp_shg, comp_shg, comp_shg, comp…\n$ exterior_1st    <fct> brk_face, vinyl_sd, wd_sdng, vinyl_sd, cemnt_bd, hd_bo…\n$ exterior_2nd    <fct> plywood, vinyl_sd, wd_sdng, vinyl_sd, cment_bd, hd_boa…\n$ mas_vnr_type    <fct> stone, none, brk_face, none, none, none, none, none, n…\n$ mas_vnr_area    <dbl> 112, 0, 108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 603, 0, 350, 0…\n$ exter_qual      <fct> ta, ta, ta, ta, gd, gd, gd, ta, ta, ta, ta, gd, ex, gd…\n$ exter_cond      <fct> ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ foundation      <fct> c_block, c_block, c_block, p_conc, p_conc, p_conc, p_c…\n$ bsmt_qual       <fct> ta, ta, ta, gd, gd, gd, gd, ta, gd, gd, gd, gd, gd, gd…\n$ bsmt_cond       <fct> gd, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ bsmt_exposure   <fct> gd, no, no, no, mn, no, no, no, no, no, gd, av, gd, av…\n$ bsmt_fin_type_1 <fct> blq, rec, alq, glq, glq, alq, glq, unf, unf, unf, glq,…\n$ bsmt_fin_sf_1   <dbl> 639, 468, 923, 791, 616, 263, 1180, 0, 0, 0, 637, 368,…\n$ bsmt_fin_type_2 <fct> unf, lw_q, unf, unf, unf, unf, unf, unf, unf, unf, unf…\n$ bsmt_fin_sf_2   <dbl> 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0, 0, 0, 0, 1…\n$ bsmt_unf_sf     <dbl> 441, 270, 406, 137, 722, 1017, 415, 994, 763, 789, 663…\n$ total_bsmt_sf   <dbl> 1080, 882, 1329, 928, 1338, 1280, 1595, 994, 763, 789,…\n$ heating         <fct> gas_a, gas_a, gas_a, gas_a, gas_a, gas_a, gas_a, gas_a…\n$ heating_qc      <fct> fa, ta, ta, gd, ex, ex, ex, gd, gd, gd, gd, ta, ex, gd…\n$ central_air     <fct> y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, …\n$ electrical      <fct> s_brkr, s_brkr, s_brkr, s_brkr, s_brkr, s_brkr, s_brkr…\n$ x1st_flr_sf     <dbl> 1656, 896, 1329, 928, 1338, 1280, 1616, 1028, 763, 789…\n$ x2nd_flr_sf     <dbl> 0, 0, 0, 701, 0, 0, 0, 776, 892, 676, 0, 0, 1589, 672,…\n$ low_qual_fin_sf <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gr_liv_area     <dbl> 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1…\n$ bsmt_full_bath  <dbl> 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, …\n$ bsmt_half_bath  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ full_bath       <dbl> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2, …\n$ half_bath       <dbl> 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, …\n$ bedroom_abv_gr  <dbl> 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 4, 4, 1, 2, 3, 3, …\n$ kitchen_abv_gr  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ kitchen_qual    <fct> ta, ta, gd, ta, gd, gd, gd, gd, ta, ta, gd, gd, ex, ta…\n$ tot_rms_abv_grd <dbl> 7, 5, 6, 6, 6, 5, 5, 7, 7, 7, 5, 4, 12, 8, 8, 4, 7, 7,…\n$ functional      <fct> typ, typ, typ, typ, typ, typ, typ, typ, typ, typ, typ,…\n$ fireplaces      <dbl> 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, …\n$ fireplace_qu    <fct> gd, none, none, ta, none, none, ta, ta, ta, gd, po, no…\n$ garage_type     <fct> attchd, attchd, attchd, attchd, attchd, attchd, attchd…\n$ garage_yr_blt   <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, …\n$ garage_finish   <fct> fin, unf, unf, fin, fin, r_fn, r_fn, fin, fin, fin, un…\n$ garage_cars     <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, …\n$ garage_area     <dbl> 528, 730, 312, 482, 582, 506, 608, 442, 440, 393, 506,…\n$ garage_qual     <fct> ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ garage_cond     <fct> ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta…\n$ paved_drive     <fct> p, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, …\n$ wood_deck_sf    <dbl> 210, 140, 393, 212, 0, 0, 237, 140, 157, 0, 192, 0, 50…\n$ open_porch_sf   <dbl> 62, 0, 36, 34, 0, 82, 152, 60, 84, 75, 0, 54, 36, 12, …\n$ enclosed_porch  <dbl> 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ x3ssn_porch     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ screen_porch    <dbl> 0, 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 140, 210, 0, 0, 0…\n$ pool_area       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ pool_qc         <fct> none, none, none, none, none, none, none, none, none, …\n$ fence           <fct> none, mn_prv, none, mn_prv, none, none, none, none, no…\n$ misc_feature    <fct> none, none, gar2, none, none, none, none, none, none, …\n$ misc_val        <dbl> 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mo_sold         <dbl> 5, 6, 6, 3, 4, 1, 3, 6, 4, 5, 2, 6, 6, 6, 6, 6, 2, 1, …\n$ yr_sold         <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, …\n$ sale_type       <fct> wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd, wd…\n$ sale_condition  <fct> normal, normal, normal, normal, normal, normal, normal…\n$ sale_price      <dbl> 215000, 105000, 172000, 189900, 213500, 191500, 236500…"
  },
  {
    "objectID": "programming.html#iteration",
    "href": "programming.html#iteration",
    "title": "8  Programming Notes",
    "section": "8.2 Iteration",
    "text": "8.2 Iteration\n\n8.2.1 across()\n\n\n8.2.2 map() and future_map()\n\n\n8.2.3 for loops\n\n\n8.2.4 foreach loops\n\n\n8.2.5 Nesting\nSome useful tutorials\n\nhttps://r4ds.had.co.nz/many-models.html\nhttps://bookdown.org/Maxine/r4ds/nesting.html\nhttps://tidyr.tidyverse.org/reference/nest.html"
  },
  {
    "objectID": "programming.html#advanced-data-wrangling",
    "href": "programming.html#advanced-data-wrangling",
    "title": "8  Programming Notes",
    "section": "8.3 Advanced data wrangling",
    "text": "8.3 Advanced data wrangling\nSee this article for details on the use of dplry for row-wise operations\nSee this article for details on the use of dplry for column-wise operations\nThe tidy folks have also written many more articles on dplyr that are worth a look"
  },
  {
    "objectID": "programming.html#tidy-programming",
    "href": "programming.html#tidy-programming",
    "title": "8  Programming Notes",
    "section": "8.4 Tidy programming",
    "text": "8.4 Tidy programming\nProgramming with dplyr can be complicated because of tidy evaluation. This programming vignette provides useful documentation on data masking and tidy selection\nIf you plan to do a lot of R programming, I highly recommend reading chapters 17-21 on meta-programming in R"
  },
  {
    "objectID": "quarto.html#general",
    "href": "quarto.html#general",
    "title": "9  Quarto",
    "section": "9.1 General",
    "text": "9.1 General\nComprehensive guide to Quarto\n\n9.1.1 Chunk options\nChunk options are now specified inside the r backtics, with the following syntax:\n#| echo: false\n#| warning: false\nTo specify these options globally, they need to be added to the _quarto.yml file, with these lines:\nexecute:\n   echo: false\n   warning: false\n\n\n9.1.2 Tables\n\n9.1.2.1 HTML\nPlain kable() renders a nicely striped table in HTML:\n\n\n\n\n\nMake\nModel\nMiles Per Gallon\nCylinders\nGears\n\n\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nMazda\nRX4\n21.0\n6\n4\n\n\nDatsun\n710\n22.8\n4\n4\n\n\nHornet\n4\n21.4\n6\n3\n\n\nHornet\nSportabout\n18.7\n8\n3\n\n\n\n\n\nHowever, when you call kableExtra, it does away with the nice striping and spacing and you then have to define that explicitly!\n\n\n\n\n \n  \n    Make \n    Model \n    Miles Per Gallon \n    Cylinders \n    Gears \n  \n \n\n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Datsun \n    710 \n    22.8 \n    4 \n    4 \n  \n  \n    Hornet \n    4 \n    21.4 \n    6 \n    3 \n  \n  \n    Hornet \n    Sportabout \n    18.7 \n    8 \n    3 \n  \n\n\n\n\n\nNote the table above, although the code is identical to the first table, has lost all formatting thanks to invoking kableExtra. The table below re-produces that via kable_styling (with additional features such as floating around text, and footnotes; although it seems the float doesn’t actually work!).\n\n\n\n\n \n  \n    Make \n    Model \n    Miles Per Gallon \n    Cylinders \n    Gears \n  \n \n\n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Mazda \n    RX4 \n    21.0 \n    6 \n    4 \n  \n  \n    Datsun \n    710 \n    22.8 \n    4 \n    4 \n  \n  \n    Hornet \n    4 \n    21.4 \n    6 \n    3 \n  \n  \n    Hornet \n    Sportabout \n    18.7 \n    8 \n    3 \n  \n\n\nNote: \n\n Here is a general comments of the table. \n\n1 Footnote 1; \n\n2 Footnote 2; \n\na Footnote A; \n\nb Footnote B; \n\n* Footnote Symbol 1; \n\n† Footnote Symbol 2\n\n\n\n\n\nAdditional HTML-only features of kableExtra are documented here\n\n\n9.1.2.2 PDF\nThe documentation for producing LaTeX tables in kableExtra is here\nThe following two tables demonstrate a complex table of the type used in the PDF ema and burden papers (here replicated with a far simpler dataset). This first table is the table code as extracted directly from the manuscript, with no additional formatting. It displays not unlike an HTML table:\n\n\n\n\nIris\n \n  \n     \n    N \n    % \n  \n \n\n  Setosa\n\n    large \n    NA \n    NA \n  \n  \n    small \n    50 \n    0.3 \n  \n  Versicolor\n\n    large \n    43 \n    0.3 \n  \n  \n    small \n    7 \n    0.0 \n  \n  Virginica\n\n    large \n    50 \n    0.3 \n  \n  \n    small \n    NA \n    NA \n  \n\n\nNote: \n\n N = 75\n\n\n\n\n\nIn LaTeX, this table displays quite differently. Below, the same code has been modified to reproduce exactly the look of the table as it appears in the PDF manuscript. That is, the code in the chunk above produces the output below when rendered with knitr to PDF; but rendering that output to HTML requires the additions in the chunk below:\n\nbootstrap_options = \"none\" added to kable_styling removes the default horizontal lines, under each row, but then font size and table width must now be specified.\npack_rows() adds horizontal lines in HTML but not LaTeX; those must be removed with the label_row_css option .\nFinally, column width is specified, and horizontal lines around the column headers and before the footnote, are added with row_spec()’s extra_css option.\n\n\n\n\n\nIris\n \n  \n     \n    N \n    % \n  \n \n\n  Setosa\n\n    large \n     \n     \n  \n  \n    small \n    50 \n    0.3 \n  \n  Versicolor\n\n    large \n    43 \n    0.3 \n  \n  \n    small \n    7 \n    0.0 \n  \n  Virginica\n\n    large \n    50 \n    0.3 \n  \n  \n    small \n     \n     \n  \n\n\nNote: \n\n N = 75\n\n\n\n\n\n\n\n9.1.2.3 Useful Table Options and features\nTo be sure tables display with NA cells as blanks (instead of “NA”) include this before your first table:\noptions(knitr.kable.NA = \"\")\nkableExtra has a function is collapse_rows, yet that seems to be persistently broken\nrow_spec() has a parameter, hline_after, which, ostensibly, should create a horizontal line under that row, right? Apparently it only works on LaTeX although this is undocumented. Apparently the solution is to add row_spec(X, extra_css = \"border-bottom: 1px solid\")) (where x is the rownum; 0 for the header, nrow(df) for the last row)\nSimilarly, to supress hlines on headers created by pack_rows(), you need to add extra_css = \"border-bottom: none\" into the pack_rows() command.\n\n\n\n9.1.3 Rendering\nHitting the Render button renders the single active document.\nCall quarto::quarto_render() to render all documents in the active project directory. NB, you must re-render the entire directory if anything changes in the .yml file (such as addition or deletion of a chapter)\nYou can specify render targets and ordering more specifically in project metadata, see https://quarto.org/docs/projects/quarto-projects.html#render-targets\n\n9.1.3.1 PDF\nYou can specify rendering to PDF format by adding the following to your .yml file:\nformat: pdf: documentclass: book\nYou can then explictly render just the PDF format by specifying both the output format and the filename in the call to quarto_render:\nquarto::quarto_render(output_format = “pdf”,output_file = “dwt.pdf”)\nHOWEVER, as of 2023_0424 rendering to PDF in the stable release fails; I’ll attempt to install the dev version and test next"
  },
  {
    "objectID": "quarto.html#documents",
    "href": "quarto.html#documents",
    "title": "9  Quarto",
    "section": "9.2 Documents",
    "text": "9.2 Documents\nWe use Quarto documents for two purposes - reproducible analyses and submitted papers. We will generally render analysis doc to html and papers to pdf. The Quarto Guide provides more detail on on creating pdf and html\nThese documents are styled primarily using Markdown. The Quarto Guide provides more detail on markdown basics"
  },
  {
    "objectID": "quarto.html#papers",
    "href": "quarto.html#papers",
    "title": "9  Quarto",
    "section": "9.3 Papers",
    "text": "9.3 Papers\nTo add a bibliography with a citation style, add the following lines to the _quarto.yml file:\nbibliography: references.bib\ncsl: name_of_csl_file.csl\nNote, we will typically call our csl files from our central lab_support repo; which simply requires the URL of the raw file from github:\ncsl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/national-library-of-medicine-grant-proposals.csl\nThe Quarto Guide provides more detail about how to work with citations and footnotes. As with markdown, the format is [@citekey1; @citekey2] — citations go inside square brackets and are separated by semicolons"
  },
  {
    "objectID": "quarto.html#reproducible-analyses",
    "href": "quarto.html#reproducible-analyses",
    "title": "9  Quarto",
    "section": "9.4 Reproducible Analyses",
    "text": "9.4 Reproducible Analyses"
  },
  {
    "objectID": "quarto.html#presentations",
    "href": "quarto.html#presentations",
    "title": "9  Quarto",
    "section": "9.5 Presentations",
    "text": "9.5 Presentations\nWe use Quarto to make revealjs slide decks for presentations. The Quarto Guide provides extensive documentation and sample slides. You can begin with the overview of presentations across formats (Quarto can also render powerpoint and other formats). Follow this with a revealjs overview and then the revealjs reference chapter. The Quarto Guide also provides a chapter on presenter tools.\nDivs (:::) and spans([]) are used extensively in presentations. An introduction to their use is provided in the markdown basics chapter. The Pandoc manual provides more detail."
  },
  {
    "objectID": "quarto.html#books",
    "href": "quarto.html#books",
    "title": "9  Quarto",
    "section": "9.6 Books",
    "text": "9.6 Books\nInformation on setting up a book: https://quarto.org/docs/books/\nInformation on setting up Github Pages to publish a book on commit: https://quarto.org/docs/publishing/github-pages.html#render-to-docs"
  },
  {
    "objectID": "quarto.html#publishing",
    "href": "quarto.html#publishing",
    "title": "9  Quarto",
    "section": "9.7 Publishing",
    "text": "9.7 Publishing\nYou can publish documents, books, and presentations to a variety of places\n\n9.7.1 Presentations and Documents\nOur preferred location for presentations (and Quarto Docs) is Quarto Pub. This site is public and free. To use it, you need to set up an account first.\nTo publish a presentation (or other Quarto doc) to Quarto Pub, you should first log in on your default browsers. You should next go to the Terminal tab in the RStudio IDE. Navigate to the folder that contains your presentation. Then type quarto publish quarto-pub. If this is the first time you are publishing at Quarto Pub on that computer, you will need to authorize it. Follow the prompts in the web browser and then in the terminal to complete the publication process. This authorization is saved in a file called _publish.yml, which will be accessed for future updates to the presentation.\nSee additional instructions in the Quarto guide if necessary.\n\n\n9.7.2 Books"
  },
  {
    "objectID": "quarto.html#terminal-commands",
    "href": "quarto.html#terminal-commands",
    "title": "9  Quarto",
    "section": "9.8 Terminal Commands",
    "text": "9.8 Terminal Commands\n\nquarto publish quarto-pub to publish a presentation or document to Quarto Pub"
  }
]